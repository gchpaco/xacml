-*- outline -*-

* To do
** dynamic
karaorman99jcontractor
** model checking
Robby-etal:TACAS2004,776863,musuvathi:osdi:cmc,786967,corbett00language,263717
** static
DBLP:conf/ecoop/2004,512558,349332,tkachuk03automated
maybe 781169?
* General
** A lot of these have to do with concurrency
*** This is because shared variable concurrency is just fucking hard
*** Global problem not always ameanable to local analysis

* 940116
.. runtime safety analysis of multithreaded programs
. : based on instrumenting bytecode of a multithreaded program to
    emit state events to observer
. : then examines messages reported by observer
. : claim to be able to do comprehensive analysis of all possible
    executions even though only one interleaving is observed?
.  , extract causal dependencies among state updates
. : closest in spirit to 694486
.  , major limitation of those systems is only analyze observed run
.. relevant causality
. : consider multithreaded systems where several threads communicate
    via shared variables
. : external observer has only instrumentation data
. : present an algorithm which, given an executing multithreaded
    system, generates appropriate messages
. : use e_i^j to represent jth event by thread p_i
. : let S be set of shared variables
.  , say e x-precedes e' e <_x e' iff e and e' are variable access
     events to same variable x and e "happens before" e'
.  , happens before can be realized by keeping a counter for every
     shared variable increased by each access
. : let \mathscr{E} be set of all events of a multithreaded execution
. : let \prec be partial order
.  , e_i^k \prec e_i^l if k < l
.  , e \prec e' if \exists x \in S so e <_x e' and at least one is a write
.  , e \prec e'' if e \prec e' and e' \prec e''
.  , write e \| e' if neither e \prec e' or e' \prec e
.  , this partial order is called a multithreaded computation
     associated with original execution
.  , synchonization can be handled by generating appropraite
     read/write events
. : premutation of events which does not violate the computation is
    called a (consistent) multithreaded run
.  , argue that this is the weakest assumption the omniscient observer
     could make
. : not everything in S is needed to be checked; subset \mathscr{R}
    \subseteq \mathscr{E} of relevant events
.  , \mathscr{R}-relevant causality on \mathscr{E} is \triangleleft
     := \pred \cap (\mathscr{R} \times \mathscr{R})
.  , give technique based on vector clocks that implements causality relation
.   ; let V_i be a length n vector of natural numbers for
      thread p_i for 1 \leq i \leq n
.   ; let V_x^a and V_x^w be two additional length n vectors;
      former is access clock, latter write clock
.   ; all initialized to 0
.   ; say V \leq V'' iff V[j] \leq V'[j] for 1 \leq j \leq n
.   ; V < V' iff V \leq V' & \exists 1 \leq j \leq n st V[j] < V'[j]
.   ; max{V,V'}[j] = max{V[j],V'[j]}
.   ; now for thread p_i with vector clock V_i processes e_i^k
.    . if e_i^k is relevant, V_i[i] \rightarrow V_i[i] + 1
.    . if read of a variable x, V_i \rightarrow max{V_i,V_x^w} and
       V_x^a \rightarrow max{V_x^a,V_i}
.    . if a write of x, V_x^w \rightarrow V_x^a \rightarrow V_i
       \rightarrow max{V_x^a,V_i}
.    . if relevant then send message e_i^k, i, V_i to observer
.   ; goal
.    . V_i[j] equals # of relevant events of p_j that causally
       precede e_i^k; if j = i and e_i^k is relevant includes it
.    . V_x^a[j] equals # of relevant events of p_j that causally
       precede most recent access event inclusive
.    . V_x^w[j] equals # of relevant write events of p_j inclusive
.    . and therefore if \<e,i,V\> and \<e',j,V'\> are two messages, e
       \triangleleft e' iff V[i] \leq V'[i]
.    . if i and j are not given, e \triangleleft e' iff V < V'
. : observer
.  , relevant multithreaded run is permutation of received events
     that doesn't violate \triangleleft
.  , assume relevant events are only writes that appear in safety formulae
.  , assume these events contain pair of name of variable & value written
.   ; (these are relevant variables)
.  , program state is mapping from relevant variables to concrete values
.  , permutation consistent iff \exists multithreaded run containing
     that state
.  , for permutation of relevant events in \mathscr{R}, e_1 e_2 \dots
     e_{|\mathscr{R}|}, e_i^k is kth even on thread p_i
.   ; so relevant program state after these events is a relevant
      global state
.   ; denoted by \Sigma^{k_1 k_2 \dots k_n} for events e_1^{k_1}
      e_2^{k_2} \dots
.  , state is consistent iff \forall 1 \leq i \leq n and \forall l_i
     \leq k_i:
.   ; \forall 1 \leq j \leq n e_j^{l_j} \triangleleft e_i^{l_i}
      \rightarrow l_j \leq k_j
.  , \Sigma^{\mathscr{K}_0} is initial global state
.   ; then for sequence of states \Sigma^{\mathscr{K}_0}
      \Sigma^{\mathscr{K}_1} \dots \Sigma^{\mathscr{K}_{|R|}}
.   ; call these runs
.  , \Sigma leads-to \Sigma' (\Sigma \leadsto \Sigma') if there is
     some run where \Sigma and \Sigma' are consecutive
.  , all consistent global states with \leadsto forms a lattice
.  , for a state \Sigma{k_1 k_2 \dots k_n}, level is \sigma_{1 \leq i
     \leq n} k_i
.  , path in lattice is sequence of consistent global states on
     increasing level
.  , so multithreaded computation is a lattice; call it computation lattice
.. safety analysis
. : use ptLTL from 694486
. : checking against single run covered there
. : safety against all runs
.  , similar, but we monitor all possible runs
.   ; this is, of course, exponential in length of computation
.   ; avoid this by traversing computation lattice level by level
.  , maintain a queue of events; whenever event arives, is enqueued
.  , also maintain set of global states current-level that are
     present in current level of lattice
.  , for each event in queue, tries to construct a global state from
     states in current level & event
.   ; if created successfully added to next-level
.  , when a global state in current level is unnecessary removed from
     current-level
.   ; when empty, this level is done and check safety formula on next-level
.   ; then move next-level to current-level and restart
.  , pseudocode:
(dolist (e queue)
  (when (match (and (memberp ?s current-level) (next-state-p ?s e)))
    (setf next-level (add-to-set next-level (create-state s e))))
  (setf current-level (remove-if #'unnecessary-p current-level))
  (unless current-level
    (monitor-all next-level)
    (setf current-level next-level
	  next-level '())
    (setf queue (remove-unnecessary-events current-level queue)))
.  , every global state s contains
.   ; values of all relevant shared variables
.   ; length n vector clock VC(s) to represent latest events from each thread
.   ; vector of flags, all set false
.    . here next-state-p checks if event e can convert s to s'
.    . pseudocode:
(defun next-state-p (s e)
  (let ((i (thread-id e)))
    (and (= (1+ (vc s i)) (vc e i))
	 (progn (setf (flags s i) t)
		(match (forall (and (<= 1 ?j n) (/= ?j i))
			       (>= (vc s j) (vc e j))))))))
.     : here n is # of threads, thread-id returns index of thread
        that generated e, etc.
.  , unnecessary-p checks if \forall 1 \leq i \leq n flags(s)[i] = true
.  , create-state creates new state, which is what happens to s when
     e is done
.   ; for monitoring have a set of pre arrays called pre-set and now
      arrays called now-set
.   ; pseudocode:
(defun create-state (s e)
  (let ((sp (clone s))
        (j (thread-id e)))
    (setf (vc sp j) (1+ (vc s j)))
    (dolist (i n)
      (setf (flags sp i) nil))
    (update-state sp e)
    (setf (pre-set sp) (now-set sp))
    sp))
.  , note that with add-to-state, if the state is already present,
     adds own PreSet to existing state's PreSet
.   ; two global states same if vector clocks equal
.  , remove-unnecessary-events removes from queue any events that
     can't contribute to construction at next level
.   ; does so by creating vector clock V_min which is the minimum of
      all corresponding components of vector clocks in current-level
.   ; then removes every event with vector clock \leq V_min
.  , monitor-all does monitoring
.   ; calls eval on each pre array to get now array
.  , for synchronization, introduce dummy shared variable
.   ; read whenever synch block starts, written after end
.   ; ensures that all events in one execution of block casually
      dependent on events in another execution of same block
.  , size of each pre array or now array is # of temporal operators
     in formula
.   ; so size of pre-set or now-set can be 2^{m'} at most
.   ; if maximum width of lattice is w, total memory required is O(w
      2^{m'})
.   ; time taken is O(w 2^m) where m size of formula
.   ; however if threads have few dependency or sync points then # of
      valid permutations can be big and so width can be big
.    . handle this by adding parameter to specify maximum width
.    . consider only most probable states there
* 694486
.. idea is to test LTL formulas on executing program
. : paper gives an algorithm to translate an LTL formula into a
    dynamic programming algorith
.  , checks whether formula is satisfied by finite trace of events in
     linear time, constant depending on LTL formula
.  , memory also constant
. : part of PathExplorer
.. post mortem
. : pull execution trace then analyze it to detect errors
.  , at this stage, this is deadlocks, races, nonconformance with LTL specs
.  , only talk about the last in this paper
. : LTL is a logic for specifying properties of reactive & concurrent systems
.  , infinite execution traces
.  , mainly used for properties of concurrent & interactive
     downscaled models
.   ; usually not scalable to real sized systems without manual abstraction
. : model checking programs has attracted interest and several
    systems exist
.  , high confidence, but scale poorly; most internal algorithms are
     at least as bad as NP complete
. : testing scales well and is widely used
.  , merge of testing & LTL specification attempt to achieve benefits
     of both
.  , also want to avoid some of the problems with adhoc testing and
     not pay for the full thing
.  , framework can only be used on single execution traces
.   ; so it can't prove something correct
.  , believe is that programmers willing to trade coverage for scalability
. : using LTL not new
.  , main theoretical contribution is showing that MaC logic & 10
     others is equivalent to past-time temporal
.  , MaC represents formula as abstract tree
.   ; evaluations done by evaluating entire tree each time
.  , suggest instead a special purpose tree traversing algorithm for
     each formula
.   ; combines general purpos algorithm with details of specific
      parse tree
.. PathExplorer
. : environment for monitoring and analyzing program execution
. : program instrumented to emit events to observer, which checks them
.  , constraints can be of different kinds defined in different languages
.  , each kind represented by a rule
.   ; in principle implements particular logic or program analysis algorithm
.  , rules for deadlock potentials, race potentials, checking
     temporal logic formulae in different logics
.   ; several rules in latter for checking future time temporal logic
      and this paper for past-time
.  , user can program new rules
. : language agnostic
. : program emits events to dispatcher which sends it to each rule
. : requires instrumentation
.  , they have an automated module for Java bytecode
.  , typically one specifies variables to be observed & what
     predicates over those variables
.. finite trace linear temporal logic
. : syntactically: (A atomic propsitions)
F ::= true | false | A | \neg F | F op F |
      \odot F | \Diamond\cdot F | \boxdot F | F_1 S_s F_2 | F_1 S_w F_2
      \uparrow F | \downarrow F | [F_1, F_2)s | [F_1, F_2)w
.  , op is standard propositional binary operators
.  , \odot F is previously F
.  , \Diamond\cdot F eventually in the past F
.  , \boxdot F always in the past F
.  , F_1 S_s F_2 F_1 strong since F_2
.  , F_1 S_w F_2 F_1 weak since F_2
.  , \uparrow F start F
.  , \downarrow F end F
.  , [F_1, F_2) is interval F_1 F_2
. : meaning
.  , trace is finite sequence of abstract states
.  , if s is a state and a an atomic propsition a(s) is true iff a
     holds in state s
.  , if t = s_1 s_2 \dots s_n is a trace t_i is the trace s_1 s_2
     \dots s_i
.  , definitions
t \models true & always true
t \models false & always false
t \models a & iff a(s_n)
t \models \neg F & iff t \nvDash F
t \models F_1 op F_2 & iff t \models F_1 and/or/implies/iff t \models
F_2 for op is \wedge/\vee/\rightarrow/\leftrightarrow
t \models \odot F & iff t' \models F where t' = t_{n-1} if n > 1 & t'
= t if n = 1
t \models \Diamond\cdot F & iff \exists 1 \leq i \leq n : t_i \models
F
t \models \boxdot F & iff \forall 1 \leq i \leq n : t_i \models F
t \models F_1 S_s F_2 & iff \exists 1 \leq j \leq n : t_j \models F_2
and \forall j < i \leq n : t_i \models F_1
t \models F_1 S_w F_2 & iff t \models F_1 S_s F_2 or t \models
\boxdot F_1
t \models \uparrow F & iff t \models F and t \nvDash \odot F
t \models \downarrow F & iff t \models \odot F and t \nvDash F
t \models [ F_1, F_2 )_s & iff \exists 1 \leq j \leq n : t_j \models
F_1 and \forall j \leq i \leq n t_i \nvDash F_2
t \models [ F_1, F_2 )_w & iff t \models [F_1, F_2)_s or t \models
\boxdot \neg F_2
.   ; note that "previously" is special on trace of one state
.   ; idea is that a trace with exactly one state s is considered
      like a stationary infinite trace containing only s
.   ; adopted due to intuitions related to monitoring
.    . may start monitoring at any moment so first state in trace may
       be different from initial state of process
.   ; could do it differently but awkward
.   ; non standard operators inspired by work in runtime verification
.    . \uparrow F is true iff F starts to be true in current state
.    . \downarrow F is true iff F stops being true in current state
.    . [F_1, F_2) true iff F_2 never true since last time F_1 true
.    . so [Start, Down)_s says server was rebooted recently and since
       has not been down
.    . [Start, Down)_w says server was not unexpectedly down recently
. : good candidate for dynamic programming due to recursive nature
. : call past time logic above ptLTL
.  , tendency for logicians to minimize number of operators required
.  , many ways to do this that are more or less irrelevant
.  , practical monitoring want to have as many operators as possible
     and not translate them
.   ; makes writing specs easier
.   ; addl memory required for each temporal operator so should keep
      formulae as concise as possible
.. example
. : \uparrow p \rightarrow [q, \downarrow (r \vee s))_s
. : need an enumeration where enumeration number of formula smaller
    than all subformulae
.  , endless ways to do this, really quite boring
. : standard way of doing dynamic programming would be to have matrix
    s[1 \dots n, 0 \dots 8] of boolean values
.  , so s[i, j] = 1 iff t_i \models \phi_j where \phi is the enumeration
.  , don't need to store all the table; need only s[i, 0\dots 8] and
     s[i-1, 0 \dots 8]
.  , call them now[0\dots 8] and pre[0 \dots 8]
. : generated program very easily done
. : complexity \Omega(n) where n is length of input trace
. : if one includes size of formula, m into analysis, time is
    \Omega(n m) memory is 2(m+1) bits
.. formalized algorithm
. : to keep presentation simple only show code for the new formulae
. : algorithm:
(defun translate-ptltl (formula)
  (let* ((subformulae (enumerate-subformulae formula))
	 (name (gensym))
	 (m (length subformulae)))
    `(defun ,name (trace)
       (let ((state '())
	     (pre (make-vector ,m nil))
	     (now (make-vector ,m nil)))
	 (setf state (update state (first trace)))
	 ,@(loop for j from m downto 0
		 for thing = `((svref ,j pre) setf)
		 for phi = (svref j subformulae)
		 do (cond
		     ((variablep phi)
		      (push 'state thing))
		     ((eq phi t)
		      (push t thing))
		     ((eq phi nil)
		      (push nil thing))
		     ((and (consp phi) (eq (first phi) 'not))
		      (push `(not (svref ,(enum-of (second phi)) pre))
			    thing))
		     ((and (consp phi) (op-p (first phi)))
		      (push `(,(first phi)
			      (svref ,(enum-of (second phi)) pre)
			      (svref ,(enum-of (third phi)) pre)) thing))
		     ((and (consp phi) (eq (first phi) 'intervals))
		      (push `(and (svref ,(enum-of (second phi)) pre)
				  (not (svref ,(enum-of (third phi)) pre)))
			    thing))
		     ((and (consp phi) (eq (first phi) 'up))
		      (push nil thing))
		     ((and (consp phi) (eq (first phi) 'down))
		      (push nil thing)))
		 collecting (nreverse thing))
	 (unless (svref 0 now)
	   (format t "~&Property violated"))
	 (dotimes (i (1- (length trace)))
	   ,@(loop for j from m downto 0
		   for thing = `((svref ,j now) setf)
		   for phi = (svref j subformulae)
		   do (cond
		       ((variablep phi)
			(push 'state thing))
		       ((eq phi t)
			(push t thing))
		       ((eq phi nil)
			(push nil thing))
		       ((and (consp phi) (eq (first phi) 'not))
			(push `(not (svref ,(enum-of (second phi)) now))
			      thing))
		       ((and (consp phi) (op-p (first phi)))
			(push `(,(first phi)
				(svref ,(enum-of (second phi)) now)
				(svref ,(enum-of (third phi)) now)) thing))
		       ((and (consp phi) (eq (first phi) 'intervals))
			(push `(and (or (svref ,j pre)
					(svref ,(enum-of (second phi)) now))
				    (not (svref ,(enum-of (third phi)) now)))
			      thing))
		       ((and (consp phi) (eq (first phi) 'up))
			(push `(and (svref ,(enum-of (second phi)) now)
				    (not (svref ,(enum-of (second phi)) pre)))
			      thing))
		       ((and (consp phi) (eq (first phi) 'down))
			(push `(and (not (svref ,(enum-of (second phi)) now))
				    (svref ,(enum-of (second phi)) pre))
			      thing)))
		   collecting (nreverse thing))
	   (unless (svref 0 now)
	     (format t "~&Property violated"))
	   (dotimes (j ,m)
	     (setf (svref j pre) (svref j now))))))))
.. implementation
. : two approaches
. : offline
.  , monitor runs in parallel with program, possibly on different computer
.  , receives events and checks on fly that formulae satisfied
. : inline
.  , formulae written as comments
.  , expanded into Java code inserted after comments
.. optimizing general code
. : code generated is not optimal, although smart compiler should be
    able to generate good machine code
. : first, not everything in pre is needed; only what's needed for
    next iteration
. : most expensive part is probably function calls for checking state
    properties
.  , could store these inline as BDDs using ?:
. : could also generate machine code
* 512538
.. partial program verification in polynomial time
. : idea is programmer gives temporal state property as FSM
. : analysis tool checks its state through the program
. : if error state never reached, program obeys property
. : previous work focused on path sensitive analysis
.  , accurate enough for verification because able to reason about
     branch correlation
.  , cost has limited applicability of this
. : present a new method that scales to large programs
. : can be expnsive because accurately tracking every branch can
    result in an exponential state space
.  , however most branches are not likely to be relevant to the property
.  , trick to ID and accurately track only relevant branches
. : present "property simulation"
.  , branch is likely to be relevant only if property FSM transitions
     to different states along the arms of it
.  , at a merge point if two symbolic states have same property
     state, merge 'em
.  , otherwise process them independently as in path sensitive analysis
. : contributions are
.  , framework for interprocedural property simulation
.  , focus on one particular instantiation where domain of execution
     states is constant propagation lattice
.   ; in this case, polynomial time & space
.  , describe ESP
.   ; uses combination of scalable alias analysis and property
      simulation to verify large code bases
.    . could also be used to provide starting point for system based
       on iterative refinement
.  , verify output file manipulations in gcc using ESP
.   ; results show property simulation is accurate
.    . able to verify that all 646 fprintf calls guaranteed to print
       to valid, open files
.   ; and scalable
.    . for each of 15 writes, able to perform interprocedural
       simulation of 140,000 LOC in 70s and 50 MB
.. property simulation
. : intraprocedural property analysis
.  , generic dataflow analysis "property analysis"
.  , computes set of possible property states at all points in a
     single procedure program
.  , assume standard CFG with
.   ; distinguished entry node n_entry
.   ; merge nodes with exactly two predecessors
.   ; branch nodes with a single predecessor, true successor and a
      false sucessor
.   ; computation nodes with single predecessor & successor
.   ; use accessor functions to get all this (In, Out, src, dst)
.  , D is finite set of states in deterministic property state machine
.   ; includes two distinguished states, $uninit initial state and
      $error error state
.  , S is domain of symbolic states
.   ; pair of abstract state (set of property states) and execution state
.    . \bot no behaviors, \top all behaviors
.   ; abstract state fn is as, execution state fn es
.  , property analysis computes for each edge in CFG dataflow fact
     from domain 2^S
.  , algorithm
(defvar *worklist*)
(defvar *info*)
(defvar *property-states*)

(defun solve (N E n-entry)
  (dolist (e E)
    (setf *info* (acons e '() *info*)))
  (setf *info* (acons (out-t n-entry) (list 'uninit 'top) *info*))
  (push (dst (out-t n-entry)) *worklist*)
  (while *worklist*
    (let ((n (first *worklist*)))
      (setf *worklist* (rest *worklist*))
      (case (type n)
	(('merge)
	 (let ((ss (f-mrg n (info (in-0 n)) (info (in-1 n)))))
	   (add (out-t n) ss)))
	(('branch)
	 (let ((ss-t (f-br n (info (in-0 n)) t))
	       (ss-f (f-br n (info (in-0 n)) nil)))
	   (add (out-t n) ss-t)
	   (add (out-f n) ss-f)))
	(('other)
	 (let ((ss (f-oth n (info (in-0 n)))))
	   (add (out-t n) ss))))))
  *info*)
(defun add (e ss)
  (when (cdr (assoc e *info*))
    (setf *info* (acons e ss *info*))
    (push (dst e) *worklist*)))
(defun f-mrg (n ss1 ss2)
  (alpha (append ss1 ss2)))
(defun f-br (n ss val)
  (alpha (remove-if (lambda (sp) (eq (es sp) 'bot))
		    (mapcar (lambda (s) (little-f-br n s val)) ss))))
(defun f-oth (n ss)
  (alpha (mapcar (lambda (s) (little-f-oth n s)) ss)))
(defun alpha-cs (ss) ss)
(defun alpha-df (ss) (list (cons (apply #'append (mapcar #'as ss))
				 (apply #'es-union (mapcar #'es ss)))))
(defun brack (ss d) (remove-if-not (lambda (s) (memberp d (as s))) ss))
(defun alpha-as (ss)
  (mapcar (lambda (d)
	    (list (cons (list d)
			(apply #'es-union (mapcar #'es (brack ss d))))))
	  (remove-if-not (lambda (d) (brack ss d)) *property-states*)))
.   ; here, little-f-br uses a theorem prover to determine whether a
      given branch is feasible with info in execution state
.   ; second if it's neither implied nor ruled out, update execution
      state setting branch predicate to true or false
.   ; little-f-oth is the transfer function for computation nodes
.    . it may update the abstract state if the node is a transition
       in the property FSM and may update execution state
.  , uses alpha to group together sets of execution states
.   ; alpha is one of the alphas given
.    . alpha-cs is fully path sensitive analysis
.    . alpha-df is standard dataflow analysis
.     : correlation between a property state and execution states
        which give rise to it is lost
.     : analysis is predicate aware (correlation between branches
        tracked as long as no merges)
.     : efficient version can be obtained by fixing execution states
        as \top
.      , this would give a path insensitive analysis that only
         propagates sets of property states
.    . alpha-as is property simulation
.     : groups elements of the set based on the property state
.  , termination and complexity
.   ; by definition of alpha-as, # of symbolic states in each
      dataflow fact is bounded by |D|
.   ; each element of set can be less precise repeatedly: at most H
      times, say
.    . can be guaranteed by requiring finite height lattice of
       execution states or using widening operators
.   ; set on a given edge can have at most |D| elements, each edge
      relaxed at most H|D| times
.   ; so O(H|E||D|) nodes processed where |E| is # of edges in CFG
.   ; their implementation keeps track of property value for which
      execution state has changed
.    . permits to evaluate f-br or f-oth only on state that has changed
.    . and evaluate \sqcup and equality test only on newly produced
       execution state
.    . so each tiem a node is processed, at most one equality
       operation, one join, one call to flow
.    . therefore complexity of intraprocedural is O(H|E||D|(T + J + Q))
.  , choice of domain for execution states and join operation left
     open; so it's a framework
.   ; can vary precision and complexity by fixing domain and join
. : instantiation for constant propagation
.  , execution states are now stores that map program variables to
     values using std constant propagation lattice
.  , join is standard join
.   ; if variable has different values in two stores, join is \top
.  , theorem prover uses execution state to replace variables in a
     predicate with values and then simplifies
.   ; if it's T or F, appropriate branch eliminated
.   ; otherwise, if it's x == c for some constant c, little-f-br
      updates execution state accordingly
.  , little-f-oth updates execution state at assignments to variables
.   ; simplifies RHS and updates store
.  , here H = 3V where V is # of variables
.  , cost of single call to theorem prover is V and so T = V
.  , join and equality take V time so J = Q = V
.  , therefore complexity is O(V^2 |E| |D|)
. : interprocedural
.  , extended in a context sensitive manner using partial transfer
     functions or summary edges
.  , idea is if processing foo and call bar, would like to apply
     transfer function from start of call to end of call in foo
.   ; don't know it yet, so need to generate it dynamically by
      analyzing bar
.   ; then cache it for future use
.  , *info* updated to associate dataflow fact at entry to bar when
     dataflow triggered
.  , may not terminate if domain of execution states is infinite as
     in constant propagation
.   ; so restrict context sensitivity to property states
.   ; treat execution states in context insensitive manner by mering
      execution states from diffferent call sites using alpha-as
.  , algorithm can be done efficiently with RHS framework
.  , complexity is O(V^2 |D| (|E| |D| + Calls |D|^2)) where Calls is
     # of call sites
.  , algorithm:
(defvar *worklist*)
(defvar *info*)
(defvar *summary*)

(defun solve (N E F n-entry)
  (dolist (f F)
    (dolist (d D)
      (setf *summary* (acons (cons f d) '() *summary*))))
  (dolist (e E)
    (dolist (d D)
      (setf *info* (acons (cons e d) '() *info*))))
  (let ((e (out-t (entry-node 'main))))
    (setf *info* (acons (cons e 'uninit) (list (cons 'uninit 'top)) *info*))
    (push (cons e 'uninit) *worklist*)
    (while *worklist*
      (let ((n (car (first *worklist*)))
	    (d (cdr (first *worklist*))))
	(setf *worklist* (rest *worklist*))
	(case (type n)
	  (('call)
	   (let ((ss-in (cdr (assoc (cons (in-0 n) d) *info*)))
		 (ss-out '()))
	     (dolist (dp D)
	       (when (brack ss-in dp)
		 (let ((value (cdr (assoc (cons (callee n) dp) *summary*))))
		   (when value
		     (setf ss-out (union ss-out value))))
		 (add-trigger (entry-node (callee n)) dp (brack ss-in dp))))
	     (add (out-t n) d (alpha ss-out))))
	  (('exit)
	   (let ((ss-in (cdr (assoc (cons (in-0 n) d) *info*))))
	     (add-to-summary n d ss-in)))
	  (('merge)
	   (let ((ss-out (f-mrg n
				(cdr (assoc (cons (in-0 n) d) *info*))
				(cdr (assoc (cons (in-1 n) d) *info*)))))
	     (add (out-t n) d ss-out)))
	  (('branch)
	   (let* ((info (cdr (assoc (cons (in-0 n) d) *info*)))
		  (ss-t (f-br n info t))
		  (ss-f (f-br n info nil)))
	     (add (out-t n) d ss-t)
	     (add (out-f n) d ss-f)))
	  (('other)
	   (let ((ss (f-oth n (cdr (assoc (cons (in-0 n) d) *info*)))))
	     (add (out-t n) d ss))))))
    *info*))
(defun add-trigger (n d ss)
  (let* ((e (out-t n))
	 (ssp (alpha (union ss (cdr (assoc (cons e d) *info*))))))
    (add e d ssp)))
(defun add (e d ss)
  (when (cdr (assoc (cons e d) *info*))
    (setf *info* (acons (cons e d) ss *info*))
    (push (cons (dst e) d) *worklist*)))
(defun add-to-summary (n d ss)
  (when (cdr (assoc (cons (fn n) d) *summary*))
    (setf *summary* (acons (cons (fn n) d) ss *summary*))
    (dolist (m (return-sites n))
      (dolist (dp D)
	(when (assoc (cons (fn m) dp) *summary*)
	  (push (cons (call-site m) dp) *worklist*))))))
.   ; here fn maps a node to name of enclosing function
.   ; entry-node maps function name to entry node
.   ; callee maps call node to name of called function
.   ; call-site maps return-site node to call-site node
.   ; return-sites maps an exit node to its return-site nodes
. : why is it precise?
.  , possible to construct programs where property simulation is less
     precise than full simulation
.  , argued to be uncommon
.  , precise because designed to match behavior of careful programmer
.. ESP
. : previous bit assumed programs update one global state machine
. : in fact usually multiple values of a given type with independent
    FSMs
. : ESP tracks states of multiple stateful values
. : partial verification that borrows key insight
.  , abstraction gap between temporal safety property and C code
     could be bridged by programmer's spec
.  , spec includes an FSM that encodes the property to be checked
.  , set of source code patterns that indicate how fragments map to
     transitions
.  , set of patterns that indicate how fresh stateful values are created
. : conservative system; if no errors, no violations
. : multiple stateful values is complicated because they flow through
    assignments to function calls
.  , therefore, ESP needs to perform global value flow analysis
.  , this determines which stateful values are affected by a given
     function call
.  , unfortunately this is intractable
. : two insights
. : first, that property analysis for multiple values can be broken
    into two sub problems
.  , use highly scalable flow-insensitive but context sensitive
     approximation of value flow
.  , then run property simulation on all the stateful values using
     previous analysis instead of tracking it during simulation
. : second, a restricted spec language to preclude properties
    correlating states of multiple variables
.  , now can analyze them one at a time
.  , like a bit-vector dataflow analysis
. : ESP is this:
.  , CFG construction using scalable points-to
.   ; every indirect call replaced with direct calls to all possible
      target functions
.   ; potentially quadratic in # of call edges, but linear in
      practice due to caching technique
.  , value flow computation
.   ; scalable context sensitive flow insensitive points-to analysis
.   ; maps every expression to a node in value flow graph VFG
.   ; conservative
.   ; VFG is context sensitive so can distinguish value flow at
      different call sites
.  , abstract CFG construction
.   ; use property specification to replace calls to pattern
      functions in CFGs with pattern nodes
.   ; each parameterized by VFG node for the expression at the call
.   ; represents possible stateful values that may have state
      transitions there
.  , interface expression computation (bottom-up slicing)
.   ; to track value flow more accurately
.   ; split up interprocedural value flow into smaller more accurate
      flows with interface expressions
.   ; input interface expression of fn bar include all globals,
      formal params of bar and dereferences of them
.    . at entry to bar, these may hold stateful values
.   ; output interface expressions all globals, return value,
      dereferences of formal parameters
.    . may hold stateful values created by bar
.   ; these can be very large due to globals
.   ; in unsafe language like C, cannot be pruned based on declared types
.   ; instead, do this:
.    . e can be ommitted from inNodes(bar) if no pattern on
       expression e' within bar or fns called by bar
.    . so that value of e flows to e' between start of bar and
       execution of pattern
.    . similarly, e can be omitted from outNodes(bar) if no value
       creation pattern on e' within bar or fns called by bar
.    . so that e' flows to e between execution of creation pattern &
       exit from bar
.   ; dramatically reduces # of inteface expressions
.   ; also use this to compute mod sets for all functions
.    . represented implicitly using VFG nodes instead of sets of variables
.    . if statement is encountered that is too complex for simluation
       engine, analysis proceeds:
.    . invalidate values in the execution state for variables in the
       mod set of the statement
.   ; alias set computation
.    . sets of interface expressions
.    . set of syntactic expressions that may hold same stateful value
.    . for input values, created during property sim
.    . output created during slicing
.    . sort of like escape analysis
.    . result is that for every created value an alias set associated
       with highest fn foo to which it escapes
.     : trigger propety analysis for these by assoc. symbolic state
	'uninit \top with s at entry to foo
.     : can discover errors that arise because value used before
        being created
.   ; propety simulation
.    . intraprocedural case
.     : suppose tracking state of alias set s
.     : if a pattern on s', query VFG
.     : if some e \in s can flow to e', update state of s based on pattern
.     : also if some e'' can flow to e', add identity transition for
        s because transition may not have occurred on value being tracked
.      , needed because VFG is only may info
.    . interprocedural case
.     : suppose tracking alias set s in foo and call bar
.      , query VFG to determine which inNodes of bar s may flow to at
	 this call site
.      , these nodes form alias set s'
.      , trigger dataflow in bar from s' in an initial state given by s
.     : suppose tracking alias set s in bar and encounter exit node
        of bar
.      , have discovered new component of summary of bar
.      , state transition d_1 \rightarrow d_2 for alias set s
.      , reflect it back to every call site c to bar by adding that
         transition at c for all alias sets in the caller
.       ; that may flow to s at c
.      , if multiple expressions from caller may flow to s at c, add
         id transition at c
.. case study file output in gcc
. : checked that fprintf arguments are open valid files in gcc
. : version from SPEC95
. : tricky because it starts by conditionally creating 15 file
    handles based on uncorrelated user flags
. : verified that no fprintf arguments are invalid
. : VFG query confirms only values on which it is called are relevant
    file handles, stdout and NULL
.  , simulation guarantees it's never called on NULL
.  , only way it can flow is by global file variables tracked by ESP
.  , that would be a bad state transition which is not the case
. : useful nontrivial property that cannot be expressed with types
. : some modifications to make it work:
.  , obstack free contains indirect call to free that pollutes call
     graph; hand modelled
.  , interprocedural property simulation context insensitive
     wrt. execution state
.   ; two file handles for which this blocks verification
.   ; avoided problem by creating two copies each of calling functions
.   ; exploring ways of introducing context-sensitivity in controlled manner
. : scalability
.  , 72.9 seconds avg runtime, 170 maximum
.  , 49.7 meg used, 102 maximum
.  , have not put significant effort into optimization
.   ; believe cost can be amortized significantly by caching states
      and reusing them across sims
.   ; and by using stnadard ordering & representation techniques from
      dataflow analysis
* yang04using
.. goal is to find file system errors using model checking
. : very dangerous sorts of errors
. : hard to reason about
. : hard to test
.  , file system must recover to correct state in event of system
     crash anywhere
.  , this led to file system stress tests
.   ; mostly focus on non crash based errors
. : they use model checking
.  , use recently developed implementation-level model checkers
.  , built on CMC
. : applied their tool to JFS, ReiserFS, ext3
.  , found 32 serious bugs
.  , most have patches within a day of diagnosis
.  , each of these have demonstrable events that lead to
     unrecoverable destruction of metadata & entire directories
.. checking overview
. : four parts:
.  , CMC running Linux kernel
.  , file system test driver
.  , permutation checker
.   ; verifies that it can recover no matter what order buffer cache
      contents written to disk
.  , fsck
. : model checker starts with empty formatted disk & generates &
    checks successive states
.  , state transitions are either test driver operations of
     FS-specific kernel threads which flush blocks to disk
. : test driver sort of like a program run
.  , creates, removes, renames files directories hard links
.  , writes to and truncates files
.  , mounts & unmounts fs
. : for each new state, intercept disk writes & forward to perm. checker
.  , it verifies that the disk is in a state that fsck can repair
.  , run fsck on host system outside of model checker
.  , use small shared library to capture all disk accesses
.  , allows it to recursively check for failures in fsck
. : permutation & fsck recovery checkers copy disk from start of
    transaction & write onto copy
.  , after copied disk is modified model checker traverses its file system
.  , records properties it checks for consistency
.   ; currently name, size, link count of every file & directory
.   ; also contents of directory
.  , model of file system data not code
.  , check that this is a valid file system and flag errors if not
. : after each new state invariant checkers run
.  , looking for FS errors
.  , if one is found, it emits an error message, stores a trace &
     discards the state
.  , if not, puts the state on the state queue if it hasn't already
     visited a similar one
. : new states checkpointed & added to state queue for later exploration
.  , checkpointing captures current execution environment of kernel
.  , kernel heap & data, disk, abstract model of current FS,
     additional invariant-related data
. : checking environment
.  , need to draw a line in the sand for external interface & fake environment
.  , in this case, driver runs on top of system call interface
.  , FiSC offers virtual block device that models disk as collection
     of sectors that can be written atomically
.   ; block device layer natural place to cut as only relatively
      well-documented boundary between core & persistent data
.  , VFS seems like a good place to cut but varies significantly
     across OSes & even different versions of kernel
.   ; system call layer eliminates numerous headaches & also permits
      checking VFS implementation
.. checking new fs
. : CMC runs Linux so anything that conforms to FiSC's assumptions of
    FS behavior doesn't need much mods to make checking work
.  , FiSC needs to know minimum disk & memory sizes FS requires
.   ; ext3 had smallest requirements: 2 MB disk, 128 pages
.  , also needs mkfs & fsck
.   ; typically three different fsck options for default recovery,
      slow full recovery, fast journal replaying
.   ; can check these against each other
.  , implementor may also need to modify FS to expose dirty blocks
.   ; important for some consistency checks
.   ; ReiserFS tracks dirty buffers by itself and needs to indicate them
. : when it fits how FiSC thought it should (ext3, JFS) takes a few
    days to start checking
.  , ReiserFS took between one and two weeks of effort
.  , it violated one of the larger assumptions
.  , FiSC mounts copy of disk used by checked file system as second
     block device
.   ; means the file system must manage two disks in a reentrant manner
.   ; ReiserFS doesn't do so; one kernel thread for performing
      journal writes for all mounted devices
.    . causes deadlock
.  , fixed problem by modifying ReiserFS not to wake the journal
     thread when a clean FS mounted read only
. : modelling FS
.  , FiSC compares checked file system against what it believes is
     correct volatile file system
.  , VolatileFS reflects all FS operations done sequentially through
     last one
.  , defined by standards rather than being FS specific
.   ; so after an operation is performed on checked concrete system,
      emulated on abstract FS, then checks that they are equivalent
.    . ignores details like time
.  , after every disk write, compares checked file system against
     what it believes is correct stable file system
.   ; StableFS reflects what FS should recover to after crash
.   ; now running fsck should alway sprovide something equivalent to StableFS
.   ; FS specific
.    . ext2 provides nearly no guarantees as to what will be recovered
.    . journaling FSes typically intend to recover up to last record
       or commit point
.    . soft-updates recovers to hard to specify mix of old & new data
.   ; determining how this evolves requires two FS specific facts
.    . when can it legally change
.     : requires implementor to modify the checked file system to call
	into the model checker to indicate when StableFS changes
.     : journaling FSes typically do this when journal commit written
.     : able to ID and annotate this for ext3 and ReiserFS easily
.     : JFS was trickier
.      , eventually gave up trying to determine which journal writes
	 were commits and let StableFS change on any journal write
.      , assume the implementor could do a better job
.    . what it changes to
.     : hard because requires writing crash recovery spec for each
        file system
.     : they run fsck instead
.      , copy experimental disk, run fsck to reconstruct fs image,
         traverse fs recording properties of interest
.      , can miss errors because no guarantee that fsck will produce
         correct state
.       ; although unlikely that fsck will fuck up when repairing
          perfect disk
.       ; even more unlikely that if it does fial it will do so in
          same way
.  , checking thoroughly
.   ; three main strategies implementer can do to check FS
      thoroughly
.   ; downscale, make everything as small as possible
.    . caches are one or two entries large
.    . FSes just a few "nodes"
.    . model checking works best on complex interactions of small
       numbers of objects
.    . three main places they did it
.     : disk is small (megabytes, not gigabytes)
.     : checking small FS topologies (2-4 nodes)
.     : reducing virtual memory of Linux system to small # of pages
.   ; canonicalization
.    . need to modify states so state hashing doesn't note irrelevant
       differences
.    . most common is to set as many things as possible to constants
.     : inode generation numbers, mount counts, time fields
.     : zeroing freed memory & unused disk blocks
.    . most require FS specific knowledge and has to be done by implementor
.    . FiSC does do two generic ones
.     : constrains search space by only writing two different values
        to blocks
.     : before hashing, FiSC removes superficial differences
.      , renames files & directories so always a sequential #ing
         among FS objects
.      , so an FS with one directory and three files "A" "B" "C" has
         same model as FS with one directory and "1" "2" "3"
.       ; if files have same length & content
.      , permits moving search space away from rare filename specific
         bugs and toward more common ones
.   ; expose choice points
.    . exposing sources of nondeterminism to FiSC
.    . low level example is adding code to fail FS-specific allocators
.    . in general any time FS makes decision on arbitrary time
       constraint or environmental feature, call FiSC
.    . mechanically this involves calling choose(n) for n alternatives
.    . example: ReiserFS & ext3 flush in-memory journals after some
       period of time
.     : replaced with choose(2)
.    . or: check buffer cache before disk reads
.     : use choose to make sure cache misses are explored
.     : particularly important for small FS topologies
.    . can exploit internal interfaces to increase set of explored actions
.     : e.g.: any invocation may give out of memory error
.     : implementation may only alloate memory sometimes, or not at all
.     : so, insert choice point to fail with out of memory
.    . not always easy to do this and may involve restructuring
.     : most invasive they made was changes to buffer cache so
        permutation checker can see all possible orderings
.. checks performed
. : generic checks
.  , deadlock
.  , dereferencing NULL
.  , paired functions not called in pairs
.   ; for example, iget iput for inode allocation
.  , memory leaks
.   ; after every state transition makes conservative traversal of
      stack & heap looking for junk
.  , no silent failures
.   ; if resource allocations fail, system call should fail
.    . except when code loops until it gets the resource
.    . generates false positive; suppress them with annotations
. : consistency checks
.  , system calls map to actions
.   ; mutations that give success should make a change
.   ; failure should make no change
.   ; use VolatileFS to make sure that change is correct one
.  , recoverable disk write ordering
.   ; arbitrary combinations of dirty buffer cache entries should
      recover to valid state
.   ; FS code frequently requires that writes happen in stylized
      orders; leads to data loss if crashes occur at bad time
.   ; doing this right requires
.    . largest legal set of possible dirty buffers
.    . flushing combinations to disk at every legal opportunity
.    . many FSes (all they checked) thwarts this by using background
       thread to write dirty blocks to disk
.     : these will not be available for reorder checking
.     : modified this thread to do nothing
.    . set initially empty
.     : blocks added whenever buffer cache entry is marked dirty
.     : removed when
.      , deleted from cache
.      , marked clean
.      , FS explicitly waits for block to be written
.      , forces synchronous write of buffer or entire request queue
.  , changed buffers marked dirty
.   ; any time FS changes block in buffer cache it needs to be marked dirty
.   ; initially thought could use generic dirty bit, but fses have
      different ideas of what that means
.    . ext3 considers buffer dirty if generic dirty bit is set
.    . or buffer is journaled & journal dirty bit is set
.    . or buffer is journaled and has been revoked and revocation valid
.   ; requires intimate knowledge of FS design and so has only been
      run on ext3
.  , buffer consistency
.   ; buffer state machine should be obeyed
.   ; e.g. ext3 blocks cannot be marked dirty & journal dirty
.  , double fsck
.   ; fsck on journaled FS that replays the journal should give same
      result as comprehensive FS
.   ; differences mean at least one wrong
.. scaling
. : exploring exponential state spaces
.  , to make it feasible you ignore "irrelevant" details and hope
     you didn't simplify too much
.  , FiSC does this twice: state hashing & searching
.  , state hashing
.   ; initially hashed most things like heap, data segment, raw disk
.   ; made it hard to explore "interesting" states comprehensively
.   ; settled on hashing VolatileFS, StableFS & currently runnable threads
.    . users can optionally hash actual disk image instead of StableFS
.   ; still can't explore all states
.    . each checkpoint is 1-3MB
.    . worklist queue consumes all memory before search space exhausted
.    . delete random states from state queue whenever size exceeds
       user selected threshold (?!)
.  , searching
.   ; provide two heuristics as well as DFS and BFS
.   ; first runs states whose disks will likely take most work to
      repair first
.    . does so by tracking how many sectors were written when
       parent's disk was recovered and sorts accordingly
.    . found a data loss error in JFS that they can't trigger with
       anything other strategy
.   ; second tries to quantify how different state is from previously
      explored states
.    . utility score based on how many times states with same
       features have been explored
.    . features include # of dirty blocks, abstract FS topology,
       whether parent executed new file statements
.    . exponentially weighted weighted sum of number of times each
       feature has been seen
. : systematically failing functions
.  , transitions make calls to functions that can fail like memory
     allocation or permission checks
.  , trying all combinations of these involve exponential number of
     uninteresting redundant transitions
.  , FS implementers relatively uninterested in weird failures like
     when malloc and a disk read both fail
.  , instead use iterative approach
.   ; first run with no failures
.   ; then run failing only a single call site until they all have
      been failed
.   ; then with two, etc.
.   ; user specified limit: default 1
. : large disks
.  , naive approach requires too much memory
.  , too hard to make it run on smaller disks
.  , instead use hash compaction
.   ; keep DB of disk chunks (collections of sectors) & their hashes
.   ; disk therefore array of references to hashed chunks
.   ; when a write alters a chunk, hash new value, put it in DB if
      necessary, and have chunk reference hash
. : fsck memoization
.  , fsck is expensive, can dominate runtimes (ext3 an order of
     magnitude longer w/o memoization, ReiserFS 2)
.  , fortunately recovery code deterministic
.  , therefore memoize calls to fsck
.   ; actually just track sectors read & written as space optimization
. : cluster-based model checking
.  , parallelize model checking
. : result: able to check 10K states and 35K transitions on ext3 in
    650 seconds
.. crashes during recovery
. : classic mistake is incorrectly handling crashes during fsck
. : this is hard to model check
.  , happens, tho: after power outage, power might die again during fsck
.  , or bad memory may cause system crash and then another one
. : check that if fsck fails, running it again should give same
    result as if first attempt succeeded
.  , do not attempt to handle fsck crashing repeatedly during recovery
.  , this is interesting, but not to implementors
. : algorithm:
.  , given disk d_0 after crash, run fsck to completion
.   ; get ordered write-list WS=(w_1, \dots, w_n) of sectors & values
      written during recovery
.  , d_i is image obtained by performing w_1, \dots, w_i to d_0.  Run
     fsck on it to perform fsck(d_i) = fsck(d_0).
.  , needs three refinements
.  , determinism
.   ; fsck is basically deterministic
.   ; so if a write doesn't change any value it previously read then
      no need to crash & rerun it
.   ; also rarely writes data it reads, those can't influence disk
.   ; ergo, two special cases exploited
.    . if w_i writes, but value is same as what's already there: skip
.     : happens more than you'd expect
.    . if sector written is dominated by earlier write and no read
       that proceeds earlier write, then skip
.  , checking all write orderings
.   ; sector writes can be really goofy unless explicitly done
      synchronously or blocked by "sync barrier"
.    . need to partition writes into sync groups and reorder them
.   ; given a sync group S_i, if size is <= to user defined
      threshold, exhaustively do it
.   ; if bigger, then pick random subset of random size
.    . seed PRNG with hash of sectors involved so it can be replayed later
.   ; usually set t = 5 and random trials to 7
.   ; without this, no bugs in fsck recovery: with it, all three FSes
      have it
.  , reasoning about errors
.   ; these are important, but reasoning is very difficult
.   ; errors are of form "wrote 17 and 38 and now disk has no files
      below /"
.   ; need to figure out what was supposed to happen, what blocks
      for, and why writing those caused problem
.    . frequently takes entire day
.    . needs to be replicated by implementor to fix it
.    . therefore want simplest possible error case
.   ; checker has five modes, roughly ordered by difficulty of diagnosing
.    . five seems a bit excessive but desperate attempt to make
       reasoning tractable
.    . synchronous atomic logical writes
.     : group sector writes into "logical writes" and do them in
        order program does
.     : easy to reason about: fsck is not reentrant
.    . synchronous nonatomic left-to-right logical
.     : logical writes synchronous, but write their contained sectors
        nonatomically left to write
.      , that is, write first sector in logical group, crash, check,
         write next, etc.
.     : also relatively easy to reason about
.     : usually means a data structure assumed to be internally
        consistent straddled sector boundary
.    . reordered atomic logical
.     : reorders all logcial writes within same sync group
.     : can often be fixed by inserting a sync call
.    . synchronous non-atomic logical writes
.     : writes sectors in logical operation in any order
.     : modular but have odd effects
.    . reordered sector writes
.     : total pain in the ass to reason about
.     : any time they find an error of this class try REALLY HARD to
        find them using something more understandable
.. results
. : all bugs reported to respective developers
. : 32 bugs total, 21 fixed & 9 of remaining 11 confirmed
.  , latter complex enough that no patch has been submitted
. : 10 errors where supposedly stable committed data & metadata
    (typically entire directories) lost
. : JFS has higher error counts in part due to immediate responses
    from developers
.  , enabled them to patch errors and continue checking it
. : unrecoverable data loss
.  , worst type
.  , in several cases all or large parts of long lived directories,
     including /, obliterated
.  , two main causes: invalid write ordering of journal & data &
     buggy implementations of transaction abort & fsck
.  , invalid write ordering:
.   ; three bugs of this type
.   ; during recovery when journal being replayed, all data modified
      by roll forward must be flushed before journal persistently cleared
.   ; otherwise if crash occurs FS will be corrupt or missing data
      but journal is empty
.   ; they all got this wrong
.  , buggy abort & fsck
.   ; five of this type, all in JFS
.   ; three causes
.   ; first, JFS immediately applies all journaled operations to in
      memory metadata pages
.    . makes it hard to roll back aborted transactions since they can
       be interleaved with writes of other ongoing or commmitted transactions
.    . so JFS relies on custom code to extricate side effects of
       aborted transactions from non aborted ones
.    . if programmer forgets to reverse a modification, can be
       flushed to disk
.   ; second, fsck makes no attempts to recover valid entries in such
      directories 
.    . if a directory contains a single invalid entry it will remove
       all entries of it to lost+found
.    . any FS mistake that persistently writes invalid entry to disk
       will blow away violated directory
.   ; third, invalid optimization
.    . implementers incorrectly believed that if FS marked dirty,
       flushing inode map unnecessary
.    . thought to be rebuilt later
.    . easy to fix but hard to find without model checker
.     : developers believe they have been chasing manifestations of
        it for a while
.  , other data loss bugs
.   ; JFS journals spanning three or more sectors can store sequence
      # in first & last but not middle
.   ; crashed FS's superblock can be falsely marked clean in JFS & ext3
.   ; JFS incorrectly stored negative error code as an inode number
      in a dirent
. : security holes
.  , didn't specifically target this, but found five of them
.  , three seem exploitable
.  , JFS hard link routine memory leaks
.  , two lookup routines in ext2 didn't distinguish between failure
     because no entry existed or malloc failure
.   ; first permits creating files or directories with same name as
      preexisting file or directory
.   ; second allows user to delete nonempty directories they don't
      have write access to
.  , remaining two in ext3 and identical to ext2 save caused by disk
     read errors
. : other bugs
.  , kernel crashes
.   ; 12 of these due to null pointer dereference
.    . most improperly handled allocation failures
.    . 1 in VFS, 1 in ReiserFS, 10 in JFS
.    . most interesting in JFS where fsck failed to correctly repair
       FS but marked it as clean; traversal panicked kernel
.   ; incorrect code
.    . sys_create creates file on disk but returns an error if
       subsequent allocation failed
.    . file still created!
.    . interesting as heavily tested VFS layer code
.   ; leaks
.    . jfs_unmount leaks memory on every unmount
.. experience
. : FiSC assisted development
.  , JFS developers prompt response permitted micro case study of
     assisted development
.   ; found and reported two kernel panics
.   ; JFS developers send fix
.   ; applied patch and verified that it fixed that problem, but full
      model checking revealed VFS segmentation faults
.    . newly created inode inserted into VFS cache before transaction committed
.    . then failed commit freed inode and left dangling pointer
.   ; sent to developers
.   ; another patch applied and then rerun: new error, where fsck
      complains that parent driectory contained invalid entry and
      blew it away
.   ; still outstanding
.  , nice properties: makes it easy to verify original error is fixed
.   ; also permits more comprehensive testing of patches
.   ; also finds corner case implications
. : false positives
.  , two groups: bugs in model checking harness or understanding of FS
.   ; have had to iteratively correct series of slight
      misunderstandings of internals of each FS
.  , other group is implementors intentionally ignoring or violating
     properties
.   ; ReiserFS causes a kernel panic when disk read fails in certain
      circumstances
.   ; easily handled by disabling the check
. : false negatives
.  , far from verification
.  , believed largest sources of missed errors
.  , exploring thresholds
.   ; do poor job of triggering system behavior after crossing a
      threshold
.   ; most glaring: since only small # of files & directories
      checked
.    . ignore directory reorganizations
.    . or directory representation change
.     : rebalancing of tree structures in JFS
.     : using a hashed directory structure in ext3
.   ; does check mixture of large & small files & filenames or dirs
      that span sector boundaries
.  , multithreading
.   ; model checker single threaded above & below system call interface
.   ; certain sorts of system oeprations never interleave
.  , white box model checking
.   ; FiSC only flags errors it sees
.   ; cannot get memory corruption, use of freed memory, races unless
      they cause a crash or invariant
.    . fortunately can simulatneously run dynamic tools
.  , unchecked guarantees
.   ; don't check some FS guarantees (versioning, undeletes, disk
      quotas, ACL, journaling of data)
.   ; one they want to fix the most is guarantees of data block contents
.    . complicated by lack of agreed upon guarantees for non-synced
       data across crashes
.  , missed states
.   ; state hashing can discard too much detail
.   ; but really they don't discard enough right details, maybe
      missing real errors
.   ; would be nice if they knew which interleaving of buffer cache
      blocks & fsck written blocks were independent
.   ; have not been aggressive about statement coverage
. : design lessons
.  , make sure inspection does not perturb state of checked system!
* 775928
.. detailed description of ckl2004's algorithm
. : so the Verilog conversion is straightforward
. : C is hard
. : going to presume knowledge of ckl2004's overview
.. so we have now a program with if instructions, assignments,
   assertions, forward gotos in SSA form (let \rho be renaming fn)
. : now want to transform it into two bit vector equations, C for
    constraints, P for assertions
. : use functions \mathscr{C}(p, g) and \mathscr{P}(p, g) where p is
    a program and g is a guard
.  , \mathscr{C}(skip, g) = \mathscr{P}(skip, g) = true
.  , \mathscr{C}(if (c) I else I', g) = \mathscr{C}(I, g \wedge
     \rho(c)) \wedge \mathscr{C}(I', g \wedge \neg \rho(c))
.  , \mathscr{P}(if (c) I else I', g) = \mathscr{P}(I, g \wedge
     \rho(c)) \wedge \mathscr{P}(I', g \wedge \neg \rho(c))
.  , \mathscr{C}(I; I', g) = \mathscr{C}(I, g) \wedge \mathscr{C}(I', g)
.  , \mathscr{P}(I; I', g) = \mathscr{P}(I, g) \wedge \mathscr{P}(I', g)
.  , \mathscr{C}(assert(a), g) = true
.  , \mathscr{P}(assert(a), g) = g \Rightarrow \rho(a)
.  , \mathscr{C}(v = e, g) = v_{new} = g ? \rho(e) : v_{old} for
     simple v
.  , \mathscr{C}(v[a] = e, g) = v_{new} \forall i : (g \wedge i =
     \rho(a)) ? \rho(e) : v_{old}[i]
.  , \mathscr{P}(v[a] = e, g) = 0 \leq \rho(a) < len(v)
. : now we need to verify that C \Rightarrow P with SAT
.. nested loops
. : can result in extremely large CNF formulas; unwinding of outer
    loop requires unwinding of inner loops too
. : convert nested loops into equivalent single loops
.  , partition body into subprograms
.  , add virtual program counter variable that keeps track of which
     is next
.  , result is monolithic loop
while (B1) {
  S1;
  while (B2) {
    S2;
  }
}
=>
while (vpc <= 2) {
  switch (vpc) {
    case 1: if (B1) { S1; vpc = 2; } else { vpc = 3; } break;
    case 2: if (B2) S2 else vpc = 1; break;
  }
}
. : reduces complexity of loop unwindings where number of times sub
    program is executed is bounding
.  , very common for specifications of synchronous hardware designs
.  , usually these use an integer to index arrays with signal values
.  , natural to increment this withing each loop
.  , can be incremented by any value and can always refer to any cycle
.   ; program is not allowed to access value of design signal at
      clock cycle greater than the bound
.   ; they expect the programmer to insert a condition on the cycle
      varible \leq to bound
.   ; so they expect a variable increased within each loop body and
      checked below the bound on every loop condition
.    . perform transformation only on such loops
.. pointers
. : remove all pointer dereferences as follows:
.  , first, &*p => p (so p = &*NULL is ok, as per ANSI)
.  , if e is subexpression to be dereferenced, remove them bottom up
.  , function \phi(e, g, o) maps pointer expression to deref.
.   ; two dereferencing operators, * and [] star uses offset zero
.   ; *e -> \phi(e, g, 0)
.   ; e[o] -> \phi(e, g, o)
.   ; pointer or array e has type *T, determined syntactically
.   ; so, what's e?
.    . e is a symbol of pointer type: let p be the pointer
.     : know that either equation generated so far or guard must have
        equality of form \rho(p) = e'
.     : dereference pointer by applying it to e': \phi(p, g, o) :=
        \phi(e', g, o)
.    . e is symbol of array type, let a be the array: syntactic sugar
       for e = &a[0]
.    . e is an address of symbol: e = &s
.     : assert offset is 0
.     : \phi(e, g, 0) = s
.     : then rename s according to SSA rules
.     : also check type consistency
.    . e is address of array element: e = &a[i]
.     : add offset to index
.     : \phi(&a[i], g, o) = a[i+o]
.     : do array access by rules above
.    . e is conditional
.     : apply \phi recursively for both cases, add condition to guard
.     : \phi(c?e':e'',g,o) = c?\phi(e',g\wedge c,o):\phi(e'',g\wedge
        \neg c, o)
.    . e is pointer arithmetic
.     : \phi(e' + i, g, o) = \phi(e', g, o+i)
.    . e is pointer typecast
.     : \phi((Q *)e', g, o) = \phi(e', g, o)
.    . anything else
.     : ANSI doesn't define semantics; e could be NULL or
        uninitialized pointer
.     : use error value then and assert that dereferencing is never executed
.     : add assertion that \rho(g) doesn't hold
.     : similar for difference p - q
.      , assert p & q point to same object
. : example
int a, b, *p;
if (x) p = &a; else p = &b;
(*p) = 1;
=>
p_1 = (x_0 ? &a : p_0) \wedge p_2 = (x_0 ? p_1 : &b)
and
(*p) = \rho(p, true, 0)
     = \rho(x_0 ? p_1 : &b, true, 0) because \rho(p) = p_2
     = x_0 ? \rho(p_1, x_0, 0) : \rho(&b, \neg x_0, 0)
     = x_0 ? \rho(x_0 ? &a : p_0, x_0, 0) : b
     = x_0 ? (x_0?\rho(&a, x_0, 0) : \rho(p_0, x_0 \wedge \neg x_0,
     0)) : b
     = x_0 ? (x_0?a : \rho(p_0, x_0 \wedge \neg x_0, 0)) : b
     = x_0 ? a : b (simplification)
. : dynamic memory allocation done with two variables
.  , active bit and object size
.  , allows bounds checks and checking whether object accessed after
     lifetime ended
.. my conclusion
. : ultimately limited due to unrolling
* ckl2004
.. use bounded model checking to reason about low level C programs
. : tool check safety properties like correctness of pointer constructs
. : can also compare C program with another design like a circuit in Verilog
. : goal is to check safety-critical legacy systems
.  , many are written in a low level language or contain ones that
     are
.  , sometimes performance requirements to do this
. : harder than verifying high level languages
.  , type safety is enforced, for example
.  , low level ANSI C has lots of arithmetic, pointers, pointer
     arithmetic, bitwise operators
. : tool checks pointer safety, array bounds, user provided assertions
. : for BMC, transition relation for a complex state machine and its
    spec fiddled with to obtain Boolean formula
.  , formula is satisfiable if there exists an error trace
.  , then run SAT on formula and extract counterexample from output
.  , tool checks that unwinding is sufficient using unwinding assertions
. : comes with GUI that hides implementation details
.  , permits stepping through the trace like it's a debugger
. : hardware verification with C as reference
.  , common hardware design is to write prototype that's like the
     circuit in C and test the crap out of it
.  , then real hardware written in things like Verilog
.  , automated consistency verification useful
.  , previous work focuses on small subset particularly close to
     register transfer language
.   ; requires a stilted sort of C
.  , this tool uses full set of ANSI C features
.  , unwind C program and circuit in tandem
.. bounded model checking
. : generating the formula
.  , since it's being reduced to determining validity of bit
     equation, need to know how to do that
.   ; full details omitted
.   ; assume that C program already preprocessed
.    . then replace side effects with equivalent statements using
       aux. variables
.    . break & continue with equivalent gotos
.    . for and do while with while loops
.   ; loop constructs unwound
.    . loops can be expressed using whiles, (recursive) functions, gotos
.    . while loops unwound by duplicating loop body n times
.     : guarding it with an if each time
.     : at last copy, add assertion ensuring program never requires more
.      , called an unwinding assertion
.      , if a loop fails for any possible execution, increase bound
         until it's big enough
.    . backward gotos unwound like while loops
.    . function calls expanded
.     : recursion handled like while loops; unwound up to a bound
.     : return statement reaplced by assignment & goto
.   ; program is now (possibly nested) ifs, assignments, assertions,
      labels, and forward goto jumps
.    . transformed into SSA using pointer analysis
.    . produces two bit-vector equations: C for constraints, P for property
.    . convert C \wedge \neg P into CNF with intermediate variables
       and pass it to SAT solver
. : converting formula to CNF
.  , conversion of most operators is straightforward
.  , resembles generation of arithmetic circuits
.  , can output bit-vector equation before it is flattened down to
     CNF for circuit level SAT solvers
.  , CMBC permits dynamic memory alloction
.   ; even though size is bounded, maximum value requires too many literals
.   ; dynamically allocated arrays are implemented with uninterpreted
      functions
.. GUI
. : goal is to make tool easily used by system designers, software
    engineers, programmers
. : works kind of like an IDE; if error trace generated, can walk
    through it like its a debugger
. : doesn't work for Verilog design, when trying to reconcile them
* ball01automatic
.. abstraction of infinite state space critical for software model checking
.. promising approach is predicate abstraction
. : concrete states are mapped to abstract states according to
    evaluation under finite set of predicates
. : algorithms have been designed & implemented for finite state
    systems
.  , also for infinite state systems specified as guarded commands
. : general task on something like C has not been demonstrated
.. present tool C2BP that performs automatic predicate abstraction of
   C programs
. : given C program P and set E of predicates
.  , predicates are pure C boolean expressions with no function calls
. : automatically creates boolean program BP(P, E) an abstraction of P
.  , boolean programs discussed in ball00bebop
.  , boolean program has same control flow structure as P but
     contains only |E| boolean variables
.   ; for example, if x < y is in E, then there is some boolean
      variable in the program for that
.  , C2BP automatically constructs corresponding boolean transfer functions
.  , resulting program can be analyzed with Bebop
. : has been applied to pointer maniuplating programs to identify
    pointer invariants
.  , these lead to more precise aliasing information than flow
     sensitive analysis can give
.  , prove structural properties of list manipulating code as well
.   ; can be done with shape analysis
.   ; notable because the predicates are quantifier free
.  , have used it to automatically identify loop invariants in three
     examples realted to proof carrying code
.  , have used it as part of SLAM to verify temporal safety of NT
     device drivers
. : detailed proof of soundness omitted
.. example
. : test partition code (like for quicksort)
. : predicates are: curr == NULL, prev == NULL, curr->val > v,
    prev->val > v
. : boolean program declares four booleans, one for each
.  , use of unknown () for unclear values to set
.  , while is superfluously nondeterministic, I think; condition
     covered by predicate
.   ; uses assumes on both branches, anyway
.   ; pretty straightforward otherwise
.   ; oh, that might be for the transition properties, of course
.    . yup
. : guarantee that every feasible execution path of the C program is
    feasible in the boolean program
.  , may be additional ones
. : after that, one of course runs Bebop on the program
.  , Bebop computes over sets of bit vectors (to capture correlations
     between variables)
.  , explicit CFG, BDDs for transfer functions & reachable states
.  , in example, Bebop says that reachable states at one stage is:
(cur \neq NULL) \wedge (curr \rightarrow val > v) \wedge ((prev
\rightarrow val \leq v) \vee (prev = NULL))
.  , this is an invariant over the state of the C program there
.  , can be used for many different purposes
.   ; one interesting one is to refine alias information
.   ; know that *prev and *curr are never aliasses at the point
.. challenges of doing it for C
. : pointers
.  , assignments through dereferenced pointers
.  , pointers and pointer dereferences in the predicates
.  , use points-to analysis to improve precision of abstraction
. : procedures
.  , handled by allowing procedural abstraction in the target language
.   ; most other approaches inline procedure calls
.  , procedures can be abstracted given the signatures of the
     abstractions of its callees
. : procedure calls 
.  , abstraction process is hard in the presence of pointers
.  , caller must conservatively update local state that can be
     modified by callee
.  , provide sound & precise approach that handles that
. : unknown values
.  , not always possible to determine what a statement will do to the
     predicates
.  , deal with that directly in target language with nondeterministic control
. : precision-efficiency tradeoff
.  , computing the abstract transfer function for each statement
     wrt. predicates may require a theorem prover
.   ; worse, O(2^E) calls to it in worst case
.  , have explored several optimization techniques to reduce calls made
.  , some have equivalent program, others trade off precision for speed
.. predicate abstraction
. : goal is a boolean program with same control structure yet only
    boolean variables
.  , guarantee that the boolean program is an abstraction of the original
. : handles all syntactic constructs
. : main limitation is it uses a logical model of memory (that is,
    p+i gives a pointer value that points to the p object)
. : assume that the C program has been converted to intermediate form where
.  , all intraprocedural control flow is done with if/then/else and gotos
.  , all expressions are free of side effects and short circuit evaluation
.  , no expressions contain multiple dereferences (**p)
.  , a function call occurs only at topmost level of expression (so
     t=f(y);x=x+t; rather than x=x+f(y))
. : WP(s, \phi) is weakest ilberal precondition of \phi with respect
    to s
.  , this is the weakest predicate whose truth before s entails truth
     of \phi after s terminates (if it does)
.  , so if x = e is a predicate, then WP(x=e,\phi) = \phi[e/x], that
     is \phi with e substitued for x
.  , this new weakest predicate may not be in E, and so a
     corresponding boolean variable may not exist
.   ; C2BP uses a theorem prover to strengthen the weakest
      precondition to something in E
.    . suppose x < 5 and x = 2 are E; then if x = 2 is true before x
       = x + 1, then x < 5 is true afterward
.   ; formalization:
A cube over V is a conjunction c_{i_1} \wedge \dots \wedge c_{i_k}
where c_{i_j} \in \{b_{i_j}, \neg b_{i_j}\} for some b_{i_j} \in V.
For a variable b_i \in V, let \mathscr{E}(b_i) is the corresponding
predicate \phi_i and \mathscr{E}(\neg b_i) be \neg \phi_i;
\mathscr{E} is extended to cubes & disjunctions of cubes in the
natural manner.  For any predicate \phi and set of boolean variables
V, \mathscr{F}_V (\phi) is the largest disjunction of cubes c over V
such that \mathscr{E}(c) implies \phi.  Then
\mathscr{E}(\mathscr{F}_V(\phi)) is the weakest predicate over
\mathscr{E}(V) that implies \phi.  In example,
\mathscr{E}(\mathscr{F}_V (x < 4)) = (x = 2).

Corresponding weakening \mathscr{G}_V is \neg\mathscr{F}_V(\neg
\phi), and \mathscr{E}(\mathscr{G}_V(\phi)) is the strongest
predicate over \mathscr{E}(V) that is implied by \phi.
.    . for each cube, implication check involves call to theorem prover
.     : C2BP uses two of them, Simplify and Vampyre
.    . naive computation of \mathscr{F}_V(\cdot) and
       \mathscr{G}_V(\cdot) can require exponentially many calls
.     : optimizations discussed later
.   ; now, with pointers WP(x=e,\phi) is not necessarily \phi[e/x]
      due to aliasing
.    . to handle this, adapt Morris' general axiom of assignment
.    . location is either a variable, structure field access, or
       dereference of location
.    . for computation of thing in question, x is a location, and let
       y be a location in \phi
.     : either x and y are aliases, and then y will become e
.     : or they are not and we're fine
.     : let \phi[x, e, y] = (&x = &y \wedge \phi[e/y]) \vee (&x \neq
        &y \wedge \phi)
.     : if y_1, y_2, \dots, y_n are locations mentioned in \phi,
        WP(x=e,\phi) = \phi[x,e,y_1][x,e,y_2]\dots[x,e,y_n]
.      , so WP(x=3,*p > 5) = (&x = p \wedge 3>5) \vee (&x \neq p
         \wedge *p > 5)
.      , if no alias information, weakest precondition will have 2^k disjuncts
.      , c2bp uses pointer analysis to improve precision using
         points-to
.       ; Das's algorithm; flow insensitive context insensitive may-alias
.   ; abstraction of assignments
.    . consider statement x = e;
.    . let l be the label of the parallel assignment of boolean
       variables in scope at l
.    . each variable b_i will be true if
       \mathscr{F}_V(WP(x=e,\phi_i)) holds before l
.    . similarly if \mathscr{F}_V(WP(x=e,\neg\phi_i)) holds before l,
       b_i will be false
.    . if neither, then it should be set to unknown
.     : can happen because E isn't strong enough or because theorem
        prover is incomplete
.    . boolean program contains parallel assignment at l:
b_1, \dots, b_n =
choose(\mathscr{F}_V(WP(x=e,\phi_1)),\mathscr{F}_V(WP(x=e,\neg\phi_1)),
\dots,
choose(\mathscr{F}_V(WP(x=e,\phi_n)),\mathscr{F}_V(WP(x=e,\neg\phi_n))
and choose is
bool choose (bool pos, bool neg) {
  if (pos) { return true; }
  if (neg) { return false; }
  return unknown ();
}
.    . show example
.    . note that abstractions are based on weakest precondition
       conditions local to assignment
.     : does not compute compositions of weakest preconditions for
        complex control flow
.     : does not require programs to have pre/post conditions or loop
        invariants
.   ; conditionals
.    . gotos get copied
.    . conditionals more involved; consider if (\phi) { \dots } else
       { \dots }
.     : at beginning of then branch, \phi is true
.      , therefore at beginning of then branch, \mathscr{G}_V(\phi) is true
.     : similarly at else branch, \neg\phi is true
.      , therefore at beginning of else branch, \mathscr{G}_V(\neg\phi) is true
.     : so boolean program looks like this:
if (*) {
   assume(\mathscr{G}_V(\phi))
   \dots
} else {
   assume(\mathscr{G}_V(\neg\phi))
   \dots
}
.     : assume is dual of assert; never fails
.   ; procedure calls
.    . G_P is globals of program
.    . each predicate in E is annotated as either global or local to
       a particular procedure
.     : global predicates refers to only variables in G_P
.    . E_G are global predicates and V_G are corresponding global
       variables in boolean program
.    . for proc R, E_R is predicates in E local to R, and V_R
       corresponding local boolean variables
.    . do not distinguish between boolean variable b and predicate
       \mathscr{E}(b) when unambiguous
.     : that is, in context of boolean program always mean b and in
        context of P always mean \mathscr{E}(b)
.    . F_R are formal parameters of R, L_R are locals, r \in L_R \cup
       F_R is return variable
.     : assume WLOG that there is only one return statement
.    . vars(e) are variables referred to in expression e
.    . drfs(e) are dereferences in e
.    . to abstract procedures in a modular fashion, need signatures
       of procedures it calls
.     : signature can be determined in isolation, given E_R
.     : therefore operate in two passes
.      , determine signatures
.      , abstract all statements
.     : if R is a procedure and R' is its abstraction, signature is
        four tuple (F_R, r, E_f, E_r)
.      , F_R is formal parameters
.      , r is return variable
.      , E_f is set of formal parameter predicates of R'
.       ; predicates in E_R that refer to no local variables of R
.       ; \{ e \in E_R | vars(e) \cap L_R = \emptyset \}
.      , E_r is return predicates
\{ e \in E_R | (r \in vars(e) \wedge (vars(e) \setminus \{r\} \cap
L_R = \emptyset)) \vee (e \in E_f \wedge (vars(e) \cap G_P \neq
\emptyset \vee drfs(e) \cap F_R \neq \emptyset)) \}
.       ; predicates returned by R'
.       ; two purposes, providing information about r and info about
          global vars and call-by-reference parameters
.      , example: if bar is
int bar(int *q, int y) {
  int l1, l2;
  ...
  return l1;
}
.       ; and predicates are y \geq 0, *q \leq y, y = l1, y > l2
.       ; E_f is \{ *q \leq y, y \geq 0 \} and E_r is \{ y = l1, *q
          \leq y \}
.     : handling the proc calls
.      , if call is v = R(a_1, \dots, a_j) at label l of procedure S
	 in P
.      , let signature of R be (F_R, r, E_f, E_r)
.      , for each e \in E_f, e' = e[a_1/f_1, \dots, a_j/f_j] where
	 F_r = \{f_1, \dots, f_j\}
.       ; e' is predicate translated to calling context
.       ; actual parameter computed is
choose(\mathscr{F}_{V_S \cup V_G}(e'), \mathscr{F}_{V_S \cup V_G}(\neg
e'))
.      , if E_r = \{e_1, \dots, e_p\}, boolean program creates p
	 fresh local variables T = \{t_1, \dots, t_p\}
.       ; then assigns in parallel the return values of R'
.      , final step is to update local predicate of S for values
	 that may have changed
.       ; any predicate in E_S that mentions v must be updated
.       ; must also update any predicates in E_S that mention
          globals
.       ; also possibly transitive dereferences to actual parameters
          and aliases for either
.       ; C2BP sues pointer alias analysis to determine
          overapproximation E_u to set of predicates to update
.       ; E' = (E_S \cup E_G) \setminus E_u
.       ; V' \subseteq V_S \cup V_G are boolean variables
          corresponding to E'
.       ; first translate predicates in E_r to calling context
\forall e_i \in E_r e'_i = e_i[v/r,a_1/f_1, \dots, a_j/f_j]
.       ; let E'_r = \{e'_1, \dots, e'_p\}
.       ; \mathscr{E}(t_i) = e'_i for t_i \in T
.       ; b \in V_S get assigned this: choose(\mathscr{F}_{V'\cup
          T}(e), \mathscr{F}_{V'\cup T}(\neg e))
.       ; in example, abstraction goes like this:
prm1 = choose({*p<=0}&&{x==0},!{*p<=0}&&{x==0});
prm2 = choose({x==0},false);
t1, t2 = bar(prm1, prm2);
{*p<=0} = choose(t1&&{x==0}, !t1&&{x==0});
{x==0} = choose(t2&&{x==0}, !t2&&{x==0});
.   ; formal properties
.    . soundness; every feasible path in P is feasible in boolean
       program as well
.     : this is somewhat trivial since all paths can be taken
.     : need to be more precise, so use abstract interpretation
.     : if p is feasible in P, p is feasible in BP as well, and
.      , if \Omega is state of C program after executing p, exists an
         execution of p in BP ending in state \Gamma st.
.      , for 1 \leq i \leq n \phi_i holds in \Omega iff b_i is true
         in \Gamma
.     : proof of this omitted
.    . precision
.     : again use abstract interpretation
.     : boolean abstraction maps concrete states to abstract states
        according to evaluation under set of predicates
.     : cartesian abstraction maps boolean vectors to three valued
        vector by ignoring dependencies between components
.      , for example cartesian abstraction of \{(0,1),(1,0)\} is
         mapped to (?,?)
.     : for single procedures without pointers, C2BP abstraction
        equiv. to composition of boolean & cartesian abstractions
.     : improve precision by using disjunctive completion and focus
        in Bebop
.. extensions
. : enforce construct
.  , if predicates in E are correlated in some way, may be impossible
     for two predicates to be simultaneously true
.  , boolean variables might, tho (uninterpreted boolean variables)
.  , to rule this out, add enforce construct:
.   ; enforce \theta puts assume \theta between every statement in procedure
.   ; \theta for each proc R is \mathscr{F}_{V_R \cup V_G} (false)
. : optimizations
.  , above method impractical without optimizations
.  , running time dominated by theorem proving, so work to cut number
     of calls down
.  , first: when computing \mathscr{F}_V(\phi), cubes considered in
     increasing order by length
.   ; if a cube c implies \phi, anything containing c as subset will
      also imply it and can be pruned
.   ; \mathscr{F} is disjunction of only prime implicants of
      \mathscr{F}_V(\phi)
.   ; similarly if it implies \neg\phi, can prune anything containing
      c as subset
.  , second for every assignment, update only variables whose truth
     value might change
.   ; will definitiely not change if WP(x=e,\mathscr{E}(b)) = \mathscr{E}(b)
.  , third, for each computation \mathscr{F}_V(\phi), produce a set
     V' \subseteq V
.   ; so that \mathscr{E}(V') contains all predicates from
      \mathscr{E}(V) that can possibly be part of a cube implying \phi
.   ; then \mathscr{F}_V(\phi) can be replaced by \mathscr{F}_{V'}(\phi)
.   ; set determined by syntactic cone-of-influence
.    . starting with empty set E' find predicates in \mathscr{E}(V)
       that mention a location or alias of location in \phi
.    . add them to E'
.    . iterate until fixpoint
.    . V' \subseteq is set of boolean variables so \mathscr{E}(V')=E'
.  , fourth, do syntactic heuristics to compute \mathscr{F}_V(\phi) directly
.   ; for example, if there's a boolean b so \mathscr{E}(b) = \phi,
      return b
.  , fifth, cache all theorem prover computations
.  , worst case analysis still exponential, above optimizations help
     most of the time
.   ; also have important property that they leave the BP
      semantically equivalent
.   ; some rely on existence of enforce statement for soundness
.  , if willing to sacrifice some precision, further opportunities
.   ; could limit lengths of cubes considered
.    . have found that limiting lengths to 3 is usually ok
.   ; could also compute \mathscr{F} only on atomic predicates
.    . \mathscr{F}(\phi_1 \wedge \phi_2) to \mathscr{F}(\phi_1)
       \wedge \mathscr{F}(\phi_2)
.    . similarly with \vee
.    . distribution through \wedge loses no precision, \vee can
.. experience
. : implemented it in OCaml on top of AST toolkit, Simplify and
    Vampyre theorem provers, and Dae's points-to
. : have aplied it to two problem areas
.  , Windows NT device drivers
.   ; goal of SLAM is to check that program respects temporal safety properties
.   ; safety properties are things that say "something bad doesn't happen"
.   ; for example, no locks are released without first being acquired
.   ; wish to either validate code or find error path
.   ; SLAM iteratively refines model to check this
.    . toolkit never reports spurious error paths, uses them to
       refine boolean program abstraction instead
.    . undecidable, so may not terminate, but seems to work
.    . properties they checked are very control intensitive and
       simple data dependencies
.   ; ran it on 4 drivers and internally developed floppy driver
.    . 4 DDK drivers are supposed to be exemplars, and SLAM found no errors
.    . floppy had an error in how interrupt requests are handled
.   ; size tops out at 6500 lines, required 5509 thm. proving calls,
      23 predicates, took 98 seconds
.   ; another with much fewer lines, more predicates, less thm prover
      calls took 93 seconds
.  , discovering array bounds and list manipulating invariants
.   ; ran it on toy examples
.    . Knuth-Morris-Pratt string matcher
.    . quicksort implementation
.    . list partition
.    . list search
.    . reverse reverses a list twice
.   ; cone-of-influence usually able to reduce number of calls to
      reasonable number
.    . except for reverse, where every pair of pointers could
       potentially alias
.    . 26769 calls for that one at 747 seconds
.   ; able to construct useful invariants by modelling only a few
      predicates that appeared
.    . in KMP and qsort only had to model bounds index \geq 0 and
       index \leq length(a) in array loops to get loop invariant
.    . usually found predicates to be guessable by looking at conditionals
.   ; reverse is a simplified version of mark & sweep GC
.    . first while loop, list traversed while maintaining back pointers
.    . second while loop, pointers reversed to get original list
.    . standard O(1) sweep
.    . wanted to verify that mark left shape of structure unchanged
.    . introduced aux. variables h and hnext into C code
.     : h points nondeterministically at any non null element of
        list, hnext is h->next
.    . with BP generated, Bebop can prove that h->next = hnext at end
       of procedure
.. future issues
. : multithreading
.  , need appropriate notion of atomicity of execution
.  , also need to account for interference
.  , also model checking boolean programs with two threads is undecidable
.   ; perhaps further abstract them to FSMs and explore interleaving executions
.  , also not possible to know # of threads in advance
. : generating predicates
.  , their program Newton does this using path simulation on counterexample
.  , also exploring oding it using value flow analysis
.  , current approach seems to work as long as properties have
     relatively simple data dependencies
.   ; more data intensive ones may have to use widening
* ball00bebop
.. symbolic model checker for boolean programs
.. boolean program is a program with control flow of something like C
   but with all variables being boolean
. : contain procedures with call-by-value parameters and recursion
. : also a restricted form of control nondeterminism
. : claimed to be interesting because:
.  , amount of storage a boolean program can access is finite
.   ; so reachability and termination are decidable (equivalent to
      push down automatons)
.  , since they have the control flow constructs of C, natural target
     for model checking of software
.   ; can be thought of as an abstract representation of C programs
.    . explicitly capture correlations between data & control
.    . boolean variables can represent arbitrary predicates over
       unbounded state of C program
.   ; therefore useful for reasoning about temporal properties
.. have created a model checker for boolean programs called Bebop
. : given a boolean program B and a statement s in B
.  , Bebop can determine if s is reachable in B
.  , then produces shortest trace leading to it
.   ; can possibly include loops & cross procedure boundaries
.   ; also reports state of variables in scope for each line
. : have adapted interprocedural dataflow analysis of Reps, Horwitz
    and Sagiv to decide reachability
.  , core idea of RHS algorithm is to compute "summaries" of
     input/output behavior of procedure
.   ; then unnecessary to reanalyze the body of a procedure if the
      input context reoccurs
.  , use BDDs to represent these summaries
. : Bebop exploits locality of variable scopes
.  , time & space complexity is O(E * 2^k)
.   ; E is # of edges in interprocedural control flow graph of
      program (linear in number of statements of program)
.   ; k is maximal number of variables in scope at any program point
.  , if # of variables in scope is held constant, running time is
     linear in terms of number of statements
.   ; have thus been able to model check boolean programs with
      thousands of lines of code & thousands of vars in a few minutes
. : also use explicit CFG representation rather than encode it with BDDs
.  , permits them to optimize model checking algorithm using compiler
     optimization techniques
.   ; explain live ranges & modification/reference analysis
.  , reduces number of variables in support of the BDDs
.. syntax
. : variables are global (outside scope of proc) or local (inside
    scope of proc)
.  , only one type so no type declarations
.  , variables are statically scoped
.  , identifier is either a C style id or arbitrary string between {
     and }
.   ; latter useful for names of predicates in another language
. : two constants, 0 and 1
. : statement sublanguage very similar to C
.  , statements can be labelled
.  , parallel assignment statement exists
.  , procedure calls use call-by-value
.   ; also support return values; can be modelled with global
      variable that gets shuffled around
.  , three statements that affect control flow: if while & assert
.   ; predicate of these is a decider
.    . either a boolean expression which evaluates deterministically
       to 0 or 1
.    . or ? which is 0 or 1 nondeterministically
.  , assign unique index to each statement in B and unique index to
     each procedure: s_i is statement of indx i
.  , to simplify presentation, assume variable names & statement
     labels are globally unique
.   ; V(B) is set of all variables in B
.   ; Globals(B) is all globals
.   ; Formals_B(i) is formal parameters of the procedure that
      contains s_i
.   ; Locals_B(i) is locals & formals of procedure that contains s_i
.    . Formals_B(i) is subset for all i up to n
.   ; InScope_B(i) is set of all variables of B whose scope includes s_i
.    . union of Locals_B(i) & Globals(B)
. : control flow graph
.  , boolean programs contain arbitrary intraprocedural control flow
     via goto
.  , useful to present semantics in terms of CFG
.  , CFG is directed graph G_B, vertices V_B= 1 ... n + p + 1,
     successor Succ_B : V_B \rightarrow 2^{V_B}
.  , V_B has an extra verted Err = n + p + 1 for failures of asserts
.  , First_B(pr) is index of first statement of procedure
.  , ProcOf_B(v) is index of procedure containg vertex v
.  , do usual CFG stuff
.   ; they require a skip after procedures for the return vertex
.    . probably not in real thing, tho
. : define transition system
.  , valuation \Omega for a subset V of V(B) associates every boolean
     variable in V with a boolean value
.   ; can be extended to expressions over V in usual way
.   ; for function f : D \rightarrow R, d \in D, r \in R, f[d/r] : D
      \rightarrow R is f[d/r](d') = r if d=d' and f(d') otherwise
.   ; state \nu of B is a pair (i, \Omega) where i \in V_B and \Omega
      is a valuation of InScope_B(i)
.   ; States(B) is all states of B
.    . intuitively contains the program counter and values for
       everything visible
.    . note we don't provide a call stack
.   ; projection \Gamma maps a state to its vertex: \Gamma(<i,
      \Omega>) = i
.    . extend \Gamma to operate on sequences of states as usual
.   ; set \Sigma(B) of terminals
\Sigma(B) = \{ \sigma \} \cup ( \<call, i, \delta\>, \<ret, i,
\delta\> : \exists j \in V_B, s_j is a procedure call, i =
ReturnPt_B(j) and \delta is a valuation to Locals_B(j)
.    . clearly finite
.   ; use \nu_1 \rightarrow^\alpha_B \nu_2 to denote an \alpha
      labelled transition from \nu_1 to \nu_2
.   ; pretty obvious, generally, except for nondeterministic choices,
      in which the PC is a set and we just operate on the set
. : now give trace semantics (yawn)
. : assume B has a distinguished procedure main, which is first that executes
.  , finite sequence \bar{\nu} is a trajectory of B if all the steps
     are legal and the labels are a statement in the induced grammar
.  , initialized trajectory if the first step is an initial state of B
.  , then the projection to vertices is a trace of B
.   ; semantics of a boolean program are the set of traces
.  , state is reachable if there exists an initialized trajectory
     that ends in it
.  , vertex is reachable if there is a trace of B that ends in it
.. reachability via interprocedural dataflow analysis & BDDs
. : redefine RHS algorithm
.  , path edges are now this:
If v is a vertex in V_B and e = First_B(ProcOf_B(v)).  A path edge
incident into a vertex v is a pair of valuations <\Omega_e, \Omega_v>
such that there is an initialized trajectory \bar{\nu_1} to \<e,
\Omega_e\> and there is a trajectory \bar{\nu_2} from \<e, \Omega_e\>
to \<v, \Omega_v\> that does not cntain the exit vertex
Exit_{ProcOf_B(v)} except for v itself.  For each vertex v,
PathEdges(v) are all path edges incident into v.
.  , summary edges are special path edges that record behavior of proc
Let c be a vertex in V_B representing a procedure call with statement
s_c = pr (e_1, e_2, ... e_k).  Summary edge associated with c is a
pair of valuations \<\Omega_1, \Omega_2\> such that all the local
variables in Locals_B(c) are equal in \Omega_1 and \Omega_2 and the
global variables change according to some path edge from the entry to
the exit of the callee.  If P is the set of path edges at Exit_pr,
Lift-c(P, pr) is the set of summary edges obtained by "lifting" the
set of path edges P to the call c while respecting semantics of
call/return.

Lift_c(P, pr) = \{\<\Omega_1, \Omega_2\> | \exists \<\Omega_i,
\Omega_o\> \in P \wedge \forall x \in Locals_B(c): \Omega_1(x) =
\Omega_2(x) \wedge \forall x \in Globals(B): (\Omega_1(x) =
\Omega_i(x)) \wedge (\Omega_2(x) = \Omega_o(x)) \wedge \forall
formals y_j of pr and actuals e_j : \Omega_1(e_j) = \Omega_i(y_j) \}
.   ; let SummaryEdges(v) be set of summary edges associated with v
.   ; as algorithm proceeds, summary edges are incrementally computed
      for each site
.   ; used to avoid revisiting portions alread explored and enable
      analysis of programs with procedures & recursion
.  , Call_b is the set of vertices in V_B that are call statements,
     Exit_B is set of exit vertices, Cond_B set of if/while/assert
.  , transfer functions
.   ; for each v where s_v \not\in Cond_B \cup Exit_B, have a
      function Transfer_v
.   ; for v \in Cond_B, transfer function Transfer_{v,true} and
      Transfer_{v,false} (definitions straightforward)
.   ; two set sof pairs of valuations S and T, Join(S,T) is image of
      set S with respect to transfer function T
Join(S, T) = \{\<\Omega_1, \Omega_2 : \exists \Omega_j. \<\Omega_1,
\Omega_j\> \in S \wedge \<\Omega_j, \Omega_2\> \in T\}
SelfLoop(S) = \{<\Omega_2,\Omega_2\> : \exists \<\Omega_1, \Omega_2\>
\in S\}
.    . SelfLoop takes path edges and makes self loops with targets of edges
.  , algorithm
(defvar *path-edges*)
(defvar *summary-edges*)
(defvar *work-list*)

(defun propagate (v p)
  (let ((edges (cdr (assoc *path-edges* v))))
    (unless (subsetp p edges)
      (setf *path-edges* (acons v (append p edges) *path-edges*))
      (pushnew v *work-list*))))
(defun readchable (b g)
  (dolist (v (vertices b))
    (push (list v) *path-edges*))
  (dolist (v (calls b))
    (push (list v) *summary-edges*))
  (let ((omega (all-valuations-of 'main)))
    (setf *path-edges* (acons (first-of b 'main) (list omega omega)
			      *path-edges*)))
  (let ((worklist (list (first-of b 'main))))
    (while worklist
      (let ((v (first worklist)))
	(setf worklist (rest worklist))
	(cond ((memberp v (calls b))
	       (propagate (ssucc b v)
			  (self-loop (join (cdr (assoc v *path-edges*))
					   (transfer v))))
	       (propagate (returnpt b v)
			  (join (cdr (assoc v *path-edges*))
				(cdr (assoc v *summary-edges*)))))
	      ((memberp v (exits b))
	       (dolist (w (succ b v))
		 (let* ((c (call-with-return b w))
			(s (lift c (cdr (assoc v *path-edges*)
					(proc-of b v))))
			(edges (cdr (assoc c *summary-edges*))))
		   (unless (subsetp s edges)
		     (setf *summary-edges* (acons c (append s edges)
						  *summary-edges*))
		     (propagate w (join (cdr (assoc c *path-edges*))
					(cdr (assoc c *summary-edges*))))))))
	      ((memberp v (conds b))
	       (propagate (tsucc b v) (join (cdr (assoc v *path-edges*))
					    (transfer-true v)))
	       (propagate (fsucc b v) (join (cdr (assoc v *path-edges*))
					    (transfer-false v))))
	      (t (let ((p (join (cdr (assoc v *path-edges*))
				(transfer v))))
		   (dolist (w (succ b v))
		     (propagate w p)))))))))
.   ; on termination, set of path edges for a vertex is empty if v is
      not reachable
.   ; if it is reachable, can generate shortest trajectory
.  , generating shorted trajectory
.   ; want to modify algorithm to keep track of length of shortest
      heirarchial trajectory required to reach each state
.   ; heirarchial trajectory can jump over procedure calls using
      summary edges
.   ; for any vertex v, let c be the first statement of the procedure
Then for a path edge \<\Omega_e, \Omega_v\>, let W(\<\Omega_e,
\Omega_v\>) be the set of heirarchial trajectories from main that
enter into the procedure of v with valuation \Omega_e and reach v with
valuation \Omega_v without exiting the procedure of v.  Heiarchial
summaries are composed of intraprocedural edges, summary edges, and
edges that represent calling a procedure (but no returns).  Partition
path edges incident on v into series of sets PathEdges_{r_1}(v)
etc. where the path edge is in PathEdges_{r_j}(v) if the shortest
heirarchial trajectory has length r_j.  Set \{r_1, r_2, ... \} is
called the set of rings associated with v.
.   ; use rings to generate shortest heirarchial trajectories
.    . if v is reachable, find smallest ring r such that
       PathEdges_r(v) exists
.    . then pick an arbitrary path edge \<\Omega_e, \Omega_v\> from it
       and do one of the following
.     : if v is not the first statement of the procedure
.      , if s_v is not a skip following a call, look for a
         predecessor u of v where
.       ; exists path edge \<\Omega_e, \Omega_u\> \in
          PathEdges_{r-1}(u) and
.       ; Join(\{\<Omega_e, \Omega_u\>\}, SummaryEdges(u)) contains
          \<\Omega_e, \Omega_v\>
.     : otherwise, e = v and \Omega_v = \Omega_e
.      , find caller u of ProcOf_B(v) and lift \Omega_v to a suitable
         pathedge in PathEdges(u) 
That is, find a vertex u \in Call_B such that s_u is a call to
procedure ProcOf_B(v) and there is a path edeg \<\Omega'_e,
\Omega_u\> in PathEdges_{r-1}(u) satisfying Transfer_u(\<\Omega_u, \Omega_v\>)
.    . repeat this process and reach main in r steps
.     : may traverse summary edges
.     : can expand them on demand
.. optimizations
. : basic algorithm above implemented in Bebop; some optimizations
    are useful
. : live ranges
.  , if for some path starting at v in CFG, x is used before being
     defined, x is live at v, otherwise it is dead at v
.   ; its value at v will not flow to any other variable
.  , if x is not live at v we do not need to record the value of x
     in the BDD for PathEdges(v)
. : mod/ref sets 
.  , modification/reference analysis determines variables
     modified/referenced by each procedure pr (and transitively)
.   ; if pr is a procedure in B such that it nor any procedures it
      calls transitively modify or reference global g
.    . even though g is in scope and may even be live, pr cannot
       change g
.    . so any summary of pr only needs to record that g is unchanged
.. evaluation
. : run it on synthetic programs from a template
.  , has a global and N+1 procedures, named main, level1, level2, ...
.  , main calls level1 twice, level1 calls level2 twice, etc.
.  , at beginning of level procedure
.   ; if g is 1 then loop executes three bit counter over locals a b c
.   ; if g is 0 then two calls made to next level
.  , at last level procedure if g is 0 two skips are executed
.  , at end of each level procedure, g is negated
. : goal here is that the program has four variables visible at any
    program point regardless of N
.  , g is not initialized so it will explore all possible values for it
. : asked Bebop to compute reachable states for all programs T(N) for
    N up to 800 and measured running time & peak memory used
.  , running time is superlinear for both (looks like)
.   ; better for BDD from CMU, CUDD is decidedly superlinear
.   ; expectation was linear
.   ; turns out to be an inefficiency in bdd_substitute in both packages
.    . if this is fixed they believe it will be linear
.  , model checking takes a minute & a half at T(800) with CMU and
     4.5 with CUDD
.   ; both times quite reasonable; T(800) has 2401 variables
.  , space measurements are slightly sublinear (taken from CUDD)
.   ; expectation was linear: sublinear behavior was due to more
      frequent GC
.. prior work
. : model checking on FSMs is well studied
. : model checking on PDAs have been studied and model checkers written
.  , they abstract away data and so spurious paths can arise
* havelund00using
.. describes how two rutime analysis algorithms have been implemented
. : race detection
. : new deadlock detection
.. also shows how they can be used to guide a model checker
. : implemented in JPF
.. model checking of actual programs has increased attention
. : most things that do it are translators
. : JPF2 model checks bytecode directly
. : major obstacle is management of large state spaces
.  , abstraction techniques studied heavily
.  , recent focus on abstraction environments for Java and C
.  , alternatives like VeriSoft & ESC tried
.   ; VeriSoft is stateless model checking of C++
.   ; ESC is of course static analysis + theorem proving
.  , static analysis entirely different discipline
.   ; remains to be seen how to handle concurrency (atomicity?)
. : alternative is runtime analysis which has only one state by definition
.  , extract important properties from one run
.  , most well known example of this applied to concurrency is Eraser
.   ; important characteristic is the run itself doesn't have to have
      a race for races to be found
.   ; cannot guarantee that errors will be found
.    . may also yield false positives
.   ; scales real nice, tho
.   ; also seems to work pretty independently of the actual run
.. work here is an extension to JPF2 to perform runtime analysis
. : either standalone or a pre-run to model checking
. : idea is that the model checking should be guided by warnings
    generated during runtime
. : implement Eraser, and also GoodLock (deadlock detection)
. : also do a third runtime dependency analysis
.  , dynamic slicing of the program befor the model checker is
     activated on runtime warnings
.. race detection uses Eraser, which we've seen before
. : implemented in the more or less straightforward manner
. : need to handle Java's reentrant locks, of course
. : don't instrument bytecodes, only JPF's simulation of them
.. deadlock detection
. : detect lock cycles
.  , need to take into account a third lock that prevents deadlocks
.  , gate lock
. : record locking pattern during runtime as a per thread lock tree
.  , when program terminates, compare the trees for each pair of threads
.  , trees built like so:
.   ; each type o is locked, do this:
(defun lock (thread lock)
  (unless (ownsp thread lock)
    (let ((son (find-son lock current)))
      (if son
	  (setf current son)
	(progn (add-son lock current)
	       (setf current (make-son))
	       (format t "~&new pattern identified"))))))
.   ; on unlock, do this:
(defun unlock (thread lock)
  (if (ownsp thread lock)
      (setf current (parent node))))
.   ; tree has at any point a current node
.    . path from root (ids thread) to that node is the lock nesting
.     : locks taken, order in which taken
.   ; lock operation creates a new child of current node if new lock
      has not previously been taken with that nesting
.   ; unlock backs up the tree if it really is released
.   ; print makes it easier to figure out when the program is
      done adding new patterns and is in a useless infinite loop
.  , trees compared with this
(defun analyse ()
  (dolist (pair (all-pairs forest))
    (let ((t1 (first pair))
	 (t2 (second pair)))
     (dolist (n1 (children t1))
       (analyze-this n1 t2)))))
(defun analyze-this (n t)
  (let ((worklist (remove-if-not (lambda (nt)
				  (and (eq (lock nt)
					   (lock n))
				       (not (below-mark nt))))
				(children t))))
    (dolist (nt worklist)
     (check n nt))
    (mapcar #'mark worklist)
    (dolist (nchild (children n))
     (analyze-this nchild t))
    (mapcar #'unmark worklist)))
(defun check (n1 n2)
  (dolist (n1child (children n1))
    (if (abovep (lock n1child) n2)
       (conflict)
     (check n1child n2))))
.   ; goal is that for every nodes n1 n2 that has the same lock, no lock
      below n1 is above n2
.   ; to avoid false alarms on gate locks, mark nodes after they've
      been examined
. : in their example they find a potential deadlock
.  , program may prevent deadlock some other way
.  , however, worth attention
.  , also: only detects deadlocks between pairs of threads
.   ; generalization possible
.   ; need to identify a subset of threads which create a conflict
.. integrating runtime analysis with model checking
. : useful as standalone tools
. : can also guide model checker
.  , first run program in simulation mode with all analysis options on
.  , threads causing warnings (race window) fed into model checker
.  , model checker will concentrate attention on the threads involved there
.  , often requires the race window to be extended to include threads
     that create or influence threads in original window
.   ; runtime dependency analysis used
. : race window extension
.  , dependency graph calculated during execution
.  , scheduler then does not schedule threads outside the window
.  , graph is a mapping from threads t to triples A, R, W
.   ; A is ancestor thread of t
.   ; R is objects t reads from
.   ; W is objects t writes to
.  , when warnings issued, add-warning called for each thread
     involved, adding it to window
.  , start-thread, read-object, write-object update graph
.   ; after program termination, used by extend-window to extend window
.   ; updated when threads start with start () method & read/write events
.  , extend-window performs fix-point calculation on all threads
     reachable
.   ; repeatedly includes threads spawned by threads in window
.   ; also includes thread shtat write to objects read by threads in window
.  , algorithms:
(defvar *window*)
(defvar *dgraph*)
(defun add-warning (thread)
  (pushnew thread *window*))
(defun start-thread (father son)
  (setf *dgraph* (acons son (list father '() '()) *dgraph*)))
(defun read-object (thread object)
  (destructuring-bind ((son ancestor read write)) (assoc thread *dgraph*)
    (setf *dgraph* (acons thread (list ancestor (cons object read) write)
			  *dgraph*))))
(defun write-object (thread object)
  (destructuring-bind ((son ancestor read write)) (assoc thread *dgraph*)
    (setf *dgraph* (acons thread (list ancestor read (cons object write))
			  *dgraph*))))
(defun extend-window (window dgraph)
  (let ((passed '())
	(waiting window))
    (while waiting
      (let ((thread (first waiting)))
	(setf waiting (rest waiting))
	(unless (member thread passed)
	  (push thread passed)
	  (destructuring-bind ((son ancestor read write))
	      (assoc thread dgraph)
	    (unless (topmostp ancestor)
	      (push ancestor waiting))
	    (dolist (threadp (all-threads))
	      (dolist (wp (fourth (assoc threadp dgraph)))
		(when (intersection wp read)
		  (push threadp waiting))))))))
    passed))
.. real example
. : remote agent; spacecraft controller written in Lisp
. : three components
.  , planner generates plans from mission goals
.  , executive executes plans
.  , recovery system monitors RA's status and suggests recovery
     actions in case of failures
. : executive is not unlike a multithreaded OS
. : planner and executive exchange messages in an interactive manner
. : ergo highly vulnerable to concurrency errors
.  , in flact in real flight the RA deadlocked while in space
.  , ground crew did locate the deadlock but asked authors if they
     could find it using model checking
. : have modelled error situation in Java; small part of RA
.  , illustrates the approach even if it isn't application to real program
. : model events and tasks
.  , event has a lock counter variable and two sync methods
.   ; one for waiting on event, one for releasing all threads waiting
.  , to catch events that occur during execution, each event has an
     event counter
.   ; incremented each time signal occurs
.  , threads only call wait_for_event if the counter is unchanged
.  , planner looks like this:
class Event {
  int count = 0;
  public synchronized void wait_for_event () {
    try{wait();} catch (InterruptedException e) {}
  }
  public synchronized void signal_event () {
    count = (count + 1) % 3;
    notifyAll ();
  }
}
class Planner extends Thread {
  Event event1, event2;
  int count = 0;

  public void run () {
    while (true) {
      if (count == event1.count)
        event1.wait_for_event ();
      count = event1.count;
      /* generate plan */
      event2.signal_event ();
    }
  }
}
.  , to illustrate integration of runtime and model checking, add
     more threads
.   ; program has 40 threads with 10,000 states each
.   ; too many for brute force checking
.  , JPF2 immediately finds data race using Eraser (count is accessed
     unsynchronized)
.   ; this might be enough to realize an error, but only if the
      consequences can be seen
.  , model checker run on thread window of all threads involved
.   ; just Planner and Executive
.   ; finds deadlock in 25 seconds
.   ; planner first does (count == event1.count), which is true
.   ; before wait_for_event, executive signals event, counter increases
.   ; planner will now unconditionally wait and miss signal
.   ; solution is to enclose conditional wait in critical section
.   ; this is what caused the deadlock in space
.. conclusions
. : claim GoodLock is useful
.  , different from Eraser in that analysis is done post mortem
. : have suggested how to use runtime analysis results to guide model checker
.  , to make smallest erroring program, do the dependency analysis
. : have also implemented Eraser by instrumenting bytecodes
.  , one of first attempts to do this, @whee
. : future work will be improving Eraser, generalizing Goodlock
.  , perhaps also alternative kinds of analysis, like wait and notify
.  , intend to investigate how to feed runtime analysis info to a slicer
* 964023
.. unlike previous work, focus on atomicity
. : usual definition familiar with now
. : absence of race conditions is not sufficient
.  , example given is StringBuffer.append (StringBuffer sb)
.  , some evidence that this is not uncommon
.  , goal is of course to reduce multithreaded behavior to sequential behavior
.  , traditional testing is inadequate
.. Atomizer is a dynamic atomicity detector
. : has been applied to over 100,000 lines of Java code
. : fewer false alarms on beign races
. : suggests that a large majority of exported methods in benchmarks
    are atomic
.  , evidence that atomicity is a useful methodology
. : benefits
.  , detecting violations that are resistant to traditional testing &
     race detection
.  , validating atomicity properties of interfaces, making safe code
     reuse easier
.  , simplifying code inspection & debugging
.  , improving concurrent programming methodology by encouraging documentation
.. first, some semantics
. : program is a number of concurrent threads with a thread ID t \in Tid
. : communicate through a global store \sigma
. : global store maps variables x to values v
.  , also records the state of each lock m \in Lock
.   ; if \sigma(m) = t then it is held by thread t
.   ; if \sigma(m) = \bot not held by any thread
. : threads also have local store \pi
. : state \Sigma = (\sigma, \Pi) has a global store \sigma and a
    mapping \Pi thread ids to local stores
. : starts at initial state \Sigma_0 = (\sigma_0, \Pi_0)
. : model behavior of each thread with transition relation T
.  , T \subseteq Tid \times LocalStore \times Operation \times LocalStore
.  , relation T(t, \pi, a \pi') holds if t can take a step from a
     state with store \pi, performing a, yielding a store \pi'
.  , possible operations on the global store includes
.   ; rd (x, v) -- reads x into v
.   ; wr (x, v) -- writes v to x
.   ; acq(m) & rel(m) -- locks
.   ; begin, end -- begins/ends an atomic block
.   ; \epsilon -- empty operation
. : proceed to define effects of operations
. : standard semantics \Sigma \rightarrow \Sigma' performs a single
    step of an arbitrarily chosen thread
.  , \rightarrow^* is the reflexive-transitive closure of \rightarrow
.  , then \Sigma_0 \rightarrow^* \Sigma models an arbitrary
     interleaving starting from \Sigma_0
.  , dynamic thread creation not directly supported but can be
     modelled straightforwardly
. : now: serialized
.  , assume A : LocalStore \rightarrow \mathbb{N} indicates the number of
     atomic blocks currently active
.   ; should be 0 in initial state and only change when entering or
      leaving an atomic block: so
.   ; A (\Pi_0 (t)) = 0 \forall t \in Tid
.   ; T (t, \pi, begin, \pi') \rightarrow A(\pi') = A(\pi) + 1
.   ; T (t, \pi, end, \pi') \rightarrow A(\pi) > 0 \wedge A(\pi') =
      A(\pi) - 1
.   ; T(t, \pi, a, \pi') \wedge a \not\in \{begin, end\} \rightarrow
      A(\pi) = A(\pi')
.  , \mathcal{A} (\pi) \equiv \exists t \in Tid . A(\Pi(t)) \neq 0
.  , serialized transition relation \mapsto is similar to \rightarrow
     but:
.   ; a thread cannot perform a step if another thread is in an atomic block
.  , reasoning is much easier with \mapsto than with \rightarrow
.  , of course, standard semantics are only for \rightarrow
.  , make an atomicity requirement
That for any execution (\sigma_0, \Pi_0) \rightarrow^* (\sigma, \Pi)
where \neg \mathcal{A}(\Pi), there should be some serialized execution
(\sigma_0, \Pi_0) \mapsto^* (\sigma, \Pi)
.. dynamic checking
. : first review reduction
.  , left mover, right mover, non mover, standard stuff
.  , can guarantee that (R+B)*N?(L+B)* is atomic
. : now, checking it:
.  , assume programmer gives a partial function P : Var
     \rightharpoonup Lock
.   ; maps protected shared variables to associated locks; if P(x) is
      undefined, x is not protected by any lock
.  , now have instrumented semantics that "goes wrong" on irreducible paths
.  , extend state space with instrumentation store: \phi : Tid
     \rightarrow \{InRight, InLeft\}
.   ; each state is now a triple (\sigma, \phi, \Pi); if A(\Pi(t))
      \neq 0 then t is in an atomic block and \phi(t) says which way
.   ; initial store \phi_0(t) = InRight \forall t \in Tid
.   ; now, keep track of it; access to a protected variable is a both
      mover when lock is held
.   ; unprotected variables are nonmovers
.    . unprotected accesses cause transition from right mover part to
       left mover part in atomic blocks
.   ; acquire are right movers; can occur in or out of atomic blocks
.   ; release are left movers; can occur in or out of atomic blocks
.  , now, \Sigma \Rightarrow^a_t wrong holds if a by thread t would
.   ; access a protected variable without holding the right lock
.   ; performing a non left mover action in left mover part of an
      atomic block
.    . accessing unprotected variable
.    . acquiring a lock
.  , important bit; if semantics permit a particular execution, then
     it is reducible and many similar ones are too
.   ; if an atomic block is being executed, only scheduling decision
      that matters is when commit is scheduled
.   ; important for testing purposes
. : so how do we infer it?
.  , use a variant of Eraser's Lockset to figure out the protecting locks
.  , extend instrumentation store to map x to candidate locks for x
.  , now we update the extended store for operations a as with Eraser
.  , if candidate lock set for a variable is empty, all accesses are non-movers
.   ; previous ones may have been incorrectly classified as both movers
.   ; to ensure soundness, lock inference doesn't support unprotected variables
.. have an implementation Atomizer
. : takes a multithreaded Java program as input and rewrites to
    include instrumentation
. : calls appropriate methods of Atomizer runtime
. : performs instrumentation on Java source code
.  , supports annotations
.  , portable across all JVMs
.  , works at high level
.  , could be performed at bytecode level
. : program includes annotations in comments to indicate that
    something should be atomic /*# atomic */
.  , several other annotations for atomic code blocks, warning
     suppression, ignoring races on some fields, etc.
.  , alternatively can apply heuristics to determine which should be checked
.   ; that is, all methods exported should be atomic
.    . exported is public or package protected
.   ; and all synchrnoized blocks & sync methods should be atomic
.   ; not used for main and run methods of Runnable objects
. : lockset implementation
.  , four states
.   ; thread local
.   ; thread local (2)
.    . second thread and ownership transferred; supports common
       initialization patterns
.   ; shared modified
.   ; shared read
.  , on field accesses, Atomizer runtime updates its state
. : reduction algorithm
.  , Atomizer implements relaxed version of semantics that accomodates
     benign race conditions
.   ; if candidate lock set becomes empty, accesses to it are non-movers
.   ; previous accesses will not be reclassified
.    . introduce a degree of unsoundness
.    . believed not to be a big deal
.     : requires strange scheduling of operations
.     : will report problem on next execution
.  , to produce clear error messages, Atomizer can capture stack
     traces at entry & commit parts of each block and include them
. : extensions
.  , can produce false alarms; imprecisions in Lockset and reduction
.   ; many techniques to eliminate some of them
.   ; reentrant locks: reentrant lock operations are both movers
.   ; thread local locks: if lock only used by one thread, lock
      operations are both movers
.   ; thread local (2) locks: to eliminate false alarms caused by
      common initialization patterns
.   ; protected locks: if thread always holds m1 before m2, two
      threads cannot attempt to get m2 at same time so both mover
.   ; write protected data: x is protected by a lock for all writes
      but no reads
.    . if it's a 32-bit variable the read is atomic despite no lock
.    . this is stickier and requires a Lockset variant
.     : algorithm infers a lock set pair of access protecting lock set
        & write protecting lock set
.     : access protecting subset of write protecting
.     : field read is a both mover if curren thread holds at least one
        of the locks, otherwise non mover
.     : field write is both mover only if access protecting lock set
        is non empty, otherwise non mover
.. evaluation
. : did not record stack histories to ensure measurements accurately
    measure cost
. : slow down varied from 2.2x to 50x; those reporting little slowdown
    spent most of their time in library code
. : believe 20x-40x are representative
. : did not focus on performance in prototype
.  , static analysies can reduce overhead of dynamic race detection to
     under 50%
.  , suggests that something similar can happen with atomicity
. : and, false alarms; frequently somewhat high
.  , did find some real bugs
.  , much higher before refinements implemented (reduces warnings by
     about 70%)
.  , significant number of false alarms due to overly optimistic
     heuristics; programmer annotations would eliminate most
.  , other sources include double checked locking, lezy
     initialization, caching idioms
.   ; notoriously problematic for race detection tools
.   ; some are incompatible with Java memory model specification
      (double checked locking)
.    . still reported as false alarms since they don't cause problems
       in current JVMs
.  , also found five fields with benign races
.  , no potential violations in over 90% of methods annotated as
     atomic
.   ; they claim this supports contention that atomicity is a
      fundamental design principle
* 265927
.. dynamic detection of data races
. : runs on real programs including the AltaVista engine
.. prior work is based on happens-before
. : checks that conflicting accesses from different threads are
    separated by sync events
.  , general but slow
. : Eraser works only on lock-based normal synch
.  , checks that all accesses follow a consistent locking discipline
.  , they argue that for many programs it is simpler, more efficient
     and more thorough than happens-before
.. definitions
. : data races occur when two threads access same memory location and
.  , one access is a write
.  , no explicit mechanism used to prevent simultaneity
.. happens-before
. : partial order on all events of all threads in a concurrent execution
. : for any single thread, events are ordered in order that they occured
. : between threads, ordered around synch objects
. : if one thread accesses a sync objects and the next is by a
    different thread, first happens before the second if
.  , semantics of the synch object forbid a schedule where these are exchanged
. : if two threads access a shared variable and they are not ordered
    by happens-before, then a race could have occured
. : two significant drawbacks
.  , hard to implement efficiently
.   ; require per-thread information about concurrent accesses to each
      memory location
.  , effectiveness highly dependent on scheduler
.. lock covers is an improvement for programs that make heavy use of locks
. : Eraser takes the improvement and discards the happens-before.
.. Lockset
. : first stab is this: every shared variable should be protected by some lock
.  , to check this, Eraser monitors all reads & writes
.  , infer the protection relation from execution history
.  , for each shared variable v, C(v) is candidate locks for v
.   ; all locks that have protected v for the computation so far
.   ; initialized to all locks
.   ; for each read/write, C(v) = intersection of locks used with C(v)
.    . called lockset refinement
.   ; if C(v) becomes empty then no lock consistently protects v
. : this is much too strict
.  , shared variables are frequently initialized without holding a lock
.  , some shared variables are initialized and thereafter read only
     (save w/o locks)
.  , read/write locks can permit multiple readers
.  , need to extend Lockset to accomodate this
. : first: initialization and read-sharing
.  , variable is done initializing when it gets accessed by a second thread
.  , also no need to protect a variable if it is read only; report
     races only after it has become write shared by more than one thread
.  , state transitions:
.   ; Virgin initial
.   ; Virgin -> Exclusive on any event
.   ; Exclusive -> Exclusive on rd/wr from first thread
.   ; Exclusive -> Shared on rd from new thread
.   ; Shared -> Shared on rd
.   ; Exclusive -> Shared-Modified on wr from new thread
.   ; Shared -> Shared-Modified on wr
.  , only warn in Shared-Modified
.  , the way this works is dependent on the scheduler more than would
     be preferable
.   ; theorized that this isn't that much of an issue but hard to be sure
. : read-write locks
.  , require that for a variable v, some lock m protects v if
.   ; m is held in write mode for ever write of v
.   ; m is held in some mode for every read of v
.  , same state transitions, but in Shared-Modified checking is
     slightly different
.   ; for reads, C(v) := C(v) \cap locks_held (t)
.   ; for writes, C(v) := C(v) \cap write_locks_held (t)
.   ; so locks in read mode are removed when a write occurs
.. implementation
. : done on Digital Unix
. : takes a binary program on input and output version w/ instrumentation
. : instruments every load and store, and every lock acquire/release
. : also thread initialization and finalization
. : for C(v) to dynamically allocated data, instruments malloc
. : every 32-bit word in the heap or global data is a possible shared variable
. : does not instrument loads/stores whose address mode is indirect of
    stack pointer
.  , deemed to be stack variables
.  , will maintain candidate sets for stack locations accessed via
     registers other than SP but this is an accident, not deliberate
. : on race report, indicates file and line number and backtrace of
    stack frames
.  , also includes thread id, memory address, type of access,
     important register values like PC and SP
.  , usually enough to locate the origin of the race
.  , can also direct Eraser to log all accesses to a particular
     variable that change its lock set
. : representing the lock sets
.  , distinct lock sets observed are quite small
.  , lock sets are a small integer, an index into a table of sets of
     locks
.   ; sets as sorted vectors of lock addresses
.  , new lockset indices created due to lock acquisitions, lock
     releases, or through intersection
.  , hash table of complete lock vectors searched before new lockset
     index is created
.  , also caches intersection results
.  , for each 32 bit word there is a shadow word that has a 30-bit
     lockset index and a 2-bit state condition
.   ; in Exclusive state, 30 bits are used to store ID of thread with
      excl. access
.  , standard malloc routines instrumented to allocate and initialize
     this shadow word
.   ; shadow word is a fixed displacement to the location's address
. : performance not a major goal so many opportunities for optimization
.  , programs typically slow down 10-30x
.  , could affect shecudling; their experience suggests it's not an issue
.  , little experience with time sensitive applications, tho
.  , half the slowdown is making a procedure call at every load & store
.   ; would be better if they could inline the code
.  , also many opportunities to use static analysis to reduce overhead
     that they have not used
.  , still fast enough to debug most programs
. : can produce false alarms
.  , memory reuse, if the program has private free lists or allocators
.  , private locks
.  , benign races
.  , to handle these, added program annotations
.   ; EraserIgnoreOn(), IgnoreOff() for benign
.   ; EraserReuse (address, size) for reused memory
.   ; EraserReadLock(lock), ReadUnlock, WriteLock, WriteUnlock
.   ; this usually sufficies to kill all false alarms
.. race detection in SPIN OS
. : SPIN's runtime code generation and late binding complicate things
    and Eraser doesn't work right yet
. : still have useful experience
. : raising and restoring interrupt level can be used as a sort of super-lock
.  , protects all data protected by lower interrupt levels
. : handle this in Eraser but assigning locks to individual interrupt levels
. : in interrupt level n, first n interrupt locks have been acquired
. : OSes also make more use of post/wait style synch
.  , semaphores are not owned and this makes it hard for Eraser to
     handle them
.. experience
. : calibrated it on a number of simple programs with common errors,
    and corrected versions
.  , extremely useful for finding bugs in Eraser
.  , also for finding bugs in "corrected" versions
. : then tackled AltaVista's HTTP server & indexing engine, Vesta
    cache server, Petal distributed disk system
.  , found undesirable races in three of the four server programs
.  , also produced false alarms
.  , ten iterations usually sufficient to resolve all reported races
.  , programmers did not intend to use Eraser or even use Eraser's
     locking discipline
.   ; this is evidence that experienced programmers tend to use simple
      locks even when others available
. : also ran it on homework problems
.  , undesirable races in many of these
. : specific examples: AltaVista
.  , many false alarms due first to memory reuse then also other things
.  , benign races are interesting because they deliberately use races
     to reduce locking overhead
.  , very tricky, though; programmer decided to recode this part even
     though it was fine
.  , some other benign races minor in scope
.  , nine annotations in indexing engine, five in test harness, ten in
     the HTTP server reduced reported races from > 100 to 0
.. Vesta cache server
. : Vesta is a software configuration management system
. : cache parts are used by builder to create particular programs
. : 30,000 lines of C++
. : tested using TestCache utility
. : Eraser reported a number of races mostly about three data structures
.  , first in code maintaining fingerpirnts in cache entries; serious
     data race
.  , another around free lists in the CacheS for log entries
.   ; tried to fix using EraserReuse, still got warnings
.   ; head of each log had a lock but the individual entries didn't
.   ; this was safe, so moved the EraserReuse annotations and they
      went away
.  , several false alarms related to server side RPCs
.   ; main thread passes current RPC socket and RPC structures to a
      worker thread; no locks needed
.   ; could, with work, modify Eraser to do this, but they just did it
      with EraserReuse
. : 10 annotations and one bug fix enough to reduce races from several
    hundred to zero
.. Petal
. : distributed storage system that presents clients with enormous
    virtual disk
. : has distributed consensus algorithm and failure detection & recovery
. : Petal server is 25,000 lines of C, tested with utility that issues
    random read/write requests
. : number of false alarms from private reader/writer lock
    implementation, suppresed w/ annotations
. : found a real race in one routine
. : two races where global variables with statistics modified w/o
    locking
.  , intentional, because locking would be too slow and statistics
     only need to be approximately correct
. : one false alarm that they couldn't make it go away
.  , one function forks a number of threads and passes a reference to
     its stack frame, then basically joins on them all to keep stack active
.  , Eraser doesn't reinitialize the shadow memory for ever stack
     frame, so reuse in different instances resulted in false alarm
.. undergraduate coursework
. : 4 standard multithreading assignments
.  , build locks from test-and-set
.  , build small threads package
.  , build semaphores and mutexes
.  , producer/consumer problems
. : run on 1000 runnable assignments (not all groups completed all
    assignments, some didn't compile, some immediately deadlocked)
.  , of the "working" assignments 10% had races found by Eraser
.   ; forgetting to take locks
.   ; taking locks during writes but not reads
.   ; using different locks for same memory object
.   ; forgetting to reacquire locks that got released in a loop
.  , also reported a false alarm from a queue that locked based on
     head/tail fields (sort of like Vesta's CacheS)
.. effectiveness
. : testing methodology, so can't be sure it's free of races
. : seems to work better than manual testing & debugging
. : also believe that it's not very sensitive to scheduler interleaving
. : to justify this, two additional experiments
. : added two races that existed in previous versions of Ni2
.  , had existed for several months before manually found
.  , using Eraser one author located them both in several minutes
     without information about what they were or where they were
.  , took about 30 minutes to fix both problems
. : also reran Ni2 and Vesta experiments with two concurrent threads
    instead of ten
.  , if it was sensitive to thread interleaving we would expect fewer reports
.  , in fact same reports (sometimes different order) across multiple runs
.. additional experience
. : protecting shared variables with multiple locks is problematic
.  , writers must own all of them, readers need only one
.  , similar to reader/writer locks but not so much aimed at
     concurrency as avoiding deadlocks
.  , earlier version of Eraser that ran on Modula-3 programs reported
     false alarms for this
.   ; changed algorithm to refine candidate set for writes at the
      time, but this can cause false negatives
.   ; theoretically possible to do this but data structures required
      make it involved
.   ; therefore current version of Eraser ignores the problem
. : deadlocks
.  , simple discipline is a partial ordering on all locks technique
.  , tried running a Modula-3 program to see if a large program known
     to have complciated synchronization did this
.   ; answer was no, revealing a potential deadlock
.   ; something along these lines might be a useful addition to Eraser
.    . needs more work, tho
* 231389
.. LCLint based
.. extend the specification language with some memory allocation concepts
.. different by being static instead of runtime based
.. have to do this in a largely intraprocedural manner
. : augmented with annotations
.. do this in the standard manner
. : assume preconditions
. : whenever a function call site is found, check its preconditions
    and signal if not met
. : still does dataflow but this simplifies things
. : some simplifying assumptions
.  , any predicate may be true or false
.  , effects of any loop are the same as running it zero or one times
.  , compile-time unknown indices or pointer offsets
.   ; either all the same element or independent elements (depends on flag)
.  , note that on many of these we can do better today
. : accept an unsound and incomplete analysis
.. storage model
. : CLU like storage model?
.  , object is a typed region of storage
.  , some are a fixed amount allocated and deallocated by compiler
.  , others are dynamic and must be managed by program
.  , undefined if it has no value, defined if it has been assigned one
.   ; completely defined if all storage reached from it is defined
. : anomaly to use undefined storage as an rvalue
. : pointer is a typed memory address; either live or dead
.  , anomaly to use dead pointers as an rvalue
.  , anomaly to dereference an allocated pointer that points to
     undefined storage
. : special null object
.  , pointers that may have null as a value is a possibly null pointer
.  , trace null propogations
. : objects have a set of owners
.  , indicates which external references can legitimately refer to this
.  , an external reference is any reference visible in the environment
     of the caller
.   ; i.e. reference to storage that can be reached from parameters,
      globals, return value
.  , owners set only includes external references and references that
     are valid to dereference
.   ; anomaly for it to be empty; memory leak
.. annotations
. : /*@ ... @*/ style in C files, bare in LCL
. : used for many specification things
.  , possibly NULL pointer
.   ; default assumption is "never null"
.   ; to aid typechecking can mark functions as returning true or
      false on nulls
.   ; can also relax it
.  , constrain instances of a type
.  , constrain use and value of function arguments and results
.  , constrain value and use of global and static declarations
.  , similar to typestates but not actually
.  , undefined or partly defined storage
.   ; default assumption is all parameters are completely defined
.   ; sometimes you may want to relax this; use out for that
.. allocation stuff
. : concerned with memory leaks, and premature freeing
. : core idea is an obligation-to-release-storage
.  , assigned to assigning reference when storage is created
.   ; storage must be freed before the reference leaves scope or it is
      assigned somewhere else
.   ; annotations indicate that this has been transferred to a return
      value, parameter or external reference
. : "only" annotation indicates that this reference is supposed to be unique
.  , also checks that any parameter as an "out only void *" should not
     contain references to live unshared objects
. : other annotations exist
.  , "temp" means that the called function may not deallocate the
     storage or create new external references
.  , "owned" is weaker than only; has an obligation to release storage
.   ; however other references with "dependent" annotations may share it
.  , additional ones for reference counted, unfreeable shared,
     internal ref exposure exist
.. aliasing
. : goal is to eliminate unexpected sharing
. : "returned" indicates that the return value may alias the parameter
. : "unique" is like only except doesn't transfer obligation to
    release storage
.  , also doesn't prevent aliasing that is invisible to the caller!
.. analysis
. : discuss a buggy program
. : doesn't handle while right; treats it as an if
.  , this is for efficiency and is sufficient for the stuff they do
     they claim
.  , their analysis notes an ownership confusion in their sample
     program
.  , also that the thing is not completely defined
.. example
. : start with no annotations
.  , default annotations are designed to be useful
. : they find some memory allocation bugs
.. experience
. : motivation is the memory management of LCLint itself
.  , original used a GC and resisted several efforts to fix it
.  , annotations helped fix it
.   ; added through an iterative process
.   ; took a few days split over several weeks to add all the
      annotations and fix detected problems
.   ; made the need for the relaxed checks clear
.   ; some places considered spurious
.    . weird memory management in error handling code
.    . several in places where the alias analysis isn't good enough
.    . execution flow analysis is not good enough to prove something impossible
.    . intentional violations deemed to be safe
.    . should be noted that one suppressed message was a real bug
.    . tested with explicit deallocation afterward
.     : didn't catch everything
.     : some bugs remained from incorrect freeing due to pointer arithmetic
.     : two from freeing static storage
.      , LCLint has been extended to fix these two
.     : two from missing annotations in the standard library
.     : one involving a complex dependency on a global
.    . after this, tried again with runtime detection
.     : found several due to stuff reachable from global or statics
.     : can't really be done without interprocedural
* 195297
** LCLint
** goal is to have a new sort of static analysis tool
. : more powerful than lint
. : less cumbersome than formal stuff
. : hidden assumptions
.  , must be local; no global program analysis
. : should be fast
. : should be flexible
. : should provide a lot of benefit even without specs and more with
. : should be easy to use
. : should be easy to start using
. : can't do much better than traditional lint without some help
.  , either stylistic decisions or partial specifications
.. warnings
. : abstraction boundaries
.  , failure to distinguish between private and public functions
     variables types
.  , direct access to abstract type
.  , code that might change if an abstract type rep was changed
.  , inappropriate type cast
.  , exposure of abstract representation
. : undocumented use of globals
. : undocumented modification of visible state
. : missing initializations
.. only a fraction of what something like this could do; still useful
.. example of incremental use
. : starts by checking for gross errors
. : proceeds to seeing if the programmer confused characters and ints,
    or ints and bools
.  , their conversion of the program to distinguish ints and bools
     finds a bug
. : move .h stuff to .lcl with a .lh stub generated by tool
.  , permits more annotations
. : making the date type abstract
. : start labelling things with globals they can use
.  , also modification
.  , catches another bug where wrong global used
. : use before definition
.  , catches a wrong variable used bug
.. real use
. : run on a small database
.  , uncovered a memory lead and a couple of abstraction violations
.  , also some false alarms because the tool isn't smart enough
. : a portfolio management tool
.  , wrote specifications of main modules and then tried to improve them
.  , uncovered several inconsistencies between implementations and specs
. : automated system builder
.  , they'd never looked at it before running the program on it
.  , found some bugs that could have been caught by conventional lint
.  , also some int/bool confusions
.  , cleaned up some abstraction violations
.  , also used LCLint to figure out which functions use/modify globals
.   ; more for documentation purposes
. : also ran it on LCLint itself
.  , helped catch some type abstraction violations
.  , helped verify that an abstract type really was
.. modern relevance?
. : some historical relation
. : tool has been upgraded several times
. : C is still pathetic
.  , less pathetic than it used to be, granted
. : many modern tool that try to do more with fewer annotations
. : LCLint's stuff is sufficiently useful that it has been folded into
    modern compilers
* 1007543
** yet another atomicity paper
** idea is that reducibility, while useful, isn't powerful enough
*** extend it by adding pure blocks
Idea behind a pure block is that its normal executions don't change
the program and so can be skipped, meaning procedure can be shown to
be atomic.
** they use reduction rules with "atomic" and "pure" block annotations
** second focus: abstract atomicity
The example they give is an alloc routine that is not actually atomic
but on the API level is atomic in that it either allocates a block or
complains that it cannot.  (on code level, in fact it is possible to
fool it) Claim that alloc is atomic under more abstract semantics;
here, the execution of the pure block is optional and so may or may
not be executed on any loop iteration.  Want to verify that every
interleaved execution of alloc has same behavior as some serial
execution.

Also support unstable variables, which are not protected by locks and
basically always racy, but we don't care because they're some sort of
packet counter that doesn't have to be exactly right.
** CAP
First reformulate C into a pseudo functional form with a native fork,
several primitives like lock stuff, arithmetic, assert, and also has
break which is "exceptional termination"; loop e evaluates e until e
breaks.  Expressions can be annotated with pure; pure e is optional
and may be skipped.  Only exceptionally terminating evaluations are
allowed to change program state.  If it temporarily changes the state
(e.g., acquires a lock) then it must restore the state before
terminating normally.  Introduce some syntactic sugar to make it look
more like C:

e1; e2           == let x = e1 in e2 (x not free in e2)
while e1 e2      == block loop { if e1 e2 break }
pure-while e1 e2 == block loop pure { if e1 e2 break }

Presume that race condition, control flow, and purity analysis is all
done; each variable has a conflict tag that is true if it may be
involved in a race condition and false otherwise, all call sites have
a call tag for the set of functions that can be invoked by the call,
and each pure expression is side effect free when executing normally;
they provide an effect system to check this in the appendix although
there are other techniques.  Also presume they have passed a type
checker.

We want to verify that every expression or procedure that is annotated
as atomic is serializable.
** effect system
Use an effect system, which is kind of like a type system but on
computations, not types.  Each expression gets an atomicity from the
set R, L, B, bot, A, top; R right commutes, L left commutes, B right
and left commutes, bot cannot terminate in that manner, A is a single
atomic action, top has none of these.  There is a partial ordering
such that bot \sqsubseteq B \sqsubseteq {L, R} \sqsubseteq A
\sqsubseteq top, with an associated join operator \sqcup.  Sequential
composition is defined according to the table.  Iterative closure a*
expresses normal termination behavior of executing a zero or more
times; bot* = B, A* = top, a* = a for others.

Define an effect environment \Gamma that maps function names to a pair
of atomicities \< a, b \> for normal and exceptional termination; also
maps primitives to a coresponding atomicity (primitives never
terminate exceptionally).

Have big effect system here that is pretty much what you would expect,
with some finessing requires near let and if.  Atomicity of variable
read depends on conflict tag: if it's false, then it is a both-mover,
if it's true (can cause conflicts), then it's A.  Effect rule for pure
expressions requires that the normal atomicity of the pure block be at
most A and so no side effects.  Since under normal termination it has
no effect, gets converted to a both mover B (exceptional different).
And atomic blocks are required to have an atomicity of A for both
termination and normal.

Any execution trace of a well typed program is equivalent to a serial
execution, so it is sound in that sense.  To verify this, first
convert each pure block into a sequence of contiguous steps and then
replace that with a skip; then reduce atomic blocks as normal and get
an equivalent serial execution.
** applications
*** double checked locking
Encloses the outer test in a pure block; only works using a
sequentially consistent memory model, note.
*** caching
Can put cache lookup in a pure block and retain atomicity.
*** wait and notify
Model wait as {release(l); acquire(l);} and notify as skip.  Wait is
not atomic.  Can use a number of equivalence rules to refactor
standard

acquire(l);
while x {
   wait(l);
}
body;
release(l);

through

e; block e'         == block e; e' if e cannot break
e; loop {e'; e}     == loop {e; e'}
break               == break; e
if e1 {e2;e} {e3;e} == {if e1 e2 e3}; e

to (note added purity statement)

block loop pure { acquire(l); if x release(l) break };
body;
release(l);

although not all wait loops can be converted like this.
*** packet counter
Handled through unstable variables.
** weak purity
Motivation is this:

atomic void apply_f () {
   int x, fx;
   weak-pure {
       acquire(m);
       x_e = z_e;
       release(m);
   }
   weak-pure-while (true) {
       fx_e = f(x_e);
       acquire(m);
       if (x_e == z_e) {
           z_e = fx_e;
	   release(m);
	   break;
       }
       x_e = z_e;
       release(m);
   }
}

Goal here is that since f takes a long time to run, we get the current
version, run f, see if z has changed, if it hasn't update, if not go.
We would like to show this is serialized with respect to other updates
to z without requiring that the lock be held while computing f.

First, classify variables as thread-local or shared.  A block can be
weak-pure if it is atomic and doesn't modify any shared variables
under normal termination; can modify thread-local variables which is
why this isn't pure.  weak-pure-while is like pure-while with
weak-pure instead.  Abstract semantics are modified to execute weak
pure statements as normal except if it accesses a shared variable then
an arbitrary value is returned (that is consistent for all accesses to
x during e).  Introduce additional execution traces and need to check
the sequential correctness of the function under the new semantics;
these semantics ensure that the blocks neither read nor modify values
of shared variables and we can treat atomic weak-pure statements as
both-movers.
* 302467
** invariants are useful
Not just for automated stuff, also for "software evolution" as
documentation.  Programmers won't do this themselves for whatever
reason, and so they propose a way to automatically detect them.
** use repeated test cases to find likely program invariants
*** dynamic
All the usual problems arise, like sensitivity to test cases, etc.; if
the test cases are not representative of the input the invariants may
be off.
** presents technique, and then results
*** running it on some programs that already have invariants
From The Science of Programming by Gries.
*** running it on some Siemens code
It helps find a bug here.
** first, an example
Tool reports all of the formally specified preconditions,
postconditions and loop invariants from Chapters 14 and 15 of the book
(14 being the first chapter where such things occur).

They first run it on an array summer that has been transliterated into
an appropriate Lisp dialect.  Instrumenter adds code that writes the
variables into a trace file for later analysis at beginning of the
program, at loop head, and at end.
*** running
Ran it on 100 random arrays of length between 7 and 13, each element
being between -100 and 100 inclusive.
*** results
Their invariant detector notes the correlation between $n$ and the
size of $b$ which was omitted in the Gries stuff.  Also picks up some
superfluous things because of the test data: all elements of $b$ are
greater than or equal to -100, $7 \leq n \leq 13$, the sum is
similarly constrained, and also some odd things: $n \neq b[-1]$ (where
$b[-1]$ denotes the last element of the array $b$, not a negative
index) and $b[0] \neq b[-1]$.  These "negative invariants" are an
artifact of the test cases used; when they rerun it later with a
exponentially distributed random set they go away.
** detection engine
Instrument source program to trace variables of interest, run the
program over several test cases and then infer invariants over the
variables recorded and also derived variables that are not present in
the original program.
*** instrumentation
Goal is to record values of variables; their prototype instruments
procedure entry and exit and loop heads, writing all variables in
scope (globals, procedure arguments, locals, return value).
Instrumenting is relatively fast, although instrumented code can
become I/O bound and slow as a result; have not attempted to optimize
this.  One could also do the checking in an online fashion which
should be nice & fast.

Each instrumentation point has a list of sets of values, one per
instrumented variable, plus a separate boolean tracking initialization
state.

Instrumenters written for Lisp and C/C++, although the latter does not
instrument loop heads yet.  Conceptually simple but requires care; for
example, in C finding the size of an array is hard, or whether a
pointer is to a variable or an array; they hand annotated it to add
all this, as well as null termination status of strings.  Also outputs
pointer addreses and contents (for pointer arithmetic stuff).

Detection requires a strange sort of test case; repeated execution of
every point or the values are not statistically useful.  Have obtained
good results so far by reusing preexisting test suites.
*** inferring
Checks the following, which was produced incrementally; started with
stuff that seemed useful and then added things deemed helpful for the
Gries programs.  Not really complete yet; no arbitrary length paths
through recursive data structures for ex.  Still useful for the
Siemens stuff.
**** any variable = constant or small number of values
**** ranges on numeric variables, also modulus (x mod b = a) and nonmodulus (x mod b != a)
**** many numbers: linear relationships (x = ay + bz + c), functions (all those in the standard library, like abs and max), comparisons (x < y), and invariants over x + y or x - y
**** sequences: sortedness, invariants over all elements
**** multiple sequences: subsequence-p, lexicographic comparison
**** sequence and scalar: membership
*** using this
For each variable or tuple, each potential invariant is checked;
invariants that fail will never be checked again, so cost is usually
proportional to the number of invariants discovered.
*** negative invariants
Stuff that might have happened but didn't.  Obviously this is
enormous.  They restrict themselves to statistically valid
conclusions; things like if $x$'s values fit into a range of size $r$
that excludes zero, probability that an instance of $x$ is not $0$ is
$1-\frac{1}{r}$.  Simplify by assuming a uniform distribution of
values; much of the tool can be seen as "statistical tests of
hypothesized distributions".  If the probability falls below a user
specified confidence interval, the negative thing is reported.  Ranges
are not reported unless believed to be noncoincidental; limit is
reported if the values near the range's extrema appear about as often
as would be expected, or if the extremum appears much more often than
expected (as if values outside the range have been clipped).

In example, the 100 samples were not enough to support any inferences
at the start or end.  These range and negative inferences were an
artifact of the test cases run.  Deliberately set probability limit to
1%; this is way too high for real work and was done to demonstrate
spurious negative invariants as shown.
*** derived variables
Several interesting classes of variables that are not used in the
program but are interesting: things like $a[i]$ where $a$ and $i$ are
variables.  These include the following.  These are treated just like
other variables, which permits discovering compound relations that are
not anticipated.  Many other potential ones are uninteresting ($x^y$,
for example).  Try not to introduce arbitrarily many new variables for
derived variables of derived variables by stopping after a fixed
number of iterations, and through other mechanisms.
**** first & last elements, length from any array
**** sum, min, max from a numeric array
**** element at index, subarray up to, subarray beyond for array and scalar
**** number of calls to function so far
*** staged derivation
Work can be avoided by looking at previous invariants; no derived
variables until invariants over preexisting variables are computed,
and introduce in stages rather than all at once; len(A) is introduced
and invariants are computed over it before any more variables are
derived; if $j \geq len(A)$, no point in looking at $A[j]$; similarly
if $j$ is only sometimes valid, and $A[0..len(A)-1] = A$ so it is not
derived.

The guaranteed relationships ($A[0] \in A$, for example) are not
reported.  If two variables are noted to be equal, one is marked as
canonical and non-canonical variables are never derived from or
further analyzed (except, presumably, if the equality becomes false).
** using invariants
They extend a Siemens regular expression matcher to do + as well as *.
Undocumented.  Initial stuff noted by inspection; distinct pattern
parser, compiler, matching engine.  Decided to compile <pat>+ to
<pat><pat>*.  Initial changes were to parse the new thing (by cloning
the parser of *) and then call the new ptclose compiler, which was a
copy of stclose initially.  Then studied function to figure out what
to do.  Ran the invariant generator and found a bug where if the
compiled pattern is too long it is bigger than a static pattern array
causing problems.  Invariant generator's role was to confirm some
intuitively determined invariants; did most of that but also noted
this problem.

Figured out how compiler worked on literals by checking the trace
database for values of the pattern buffer at the entry & exit of
stclose; this gave an important clue.

Additional invariants on makepat helped prevent introduction of bugs
(they cite a relationship they initially believed to exist between
\texttt{j} and \texttt{lj} which turned out to be false).
*** analysis
This was useful, but ultimately could have been discovered by
carefully reading the code, additional static stuff, and some
instrumentation.  The invariants are still useful.

They provide a nice, programmer friendly description of the entire
trace database.  They may be incomplete or later falsified by
additional stuff, but they still provide insight.  Also, queries on
the DB provide a place to start when unexpected invariants are found,
or when expected invariants are not found.  Can also be used to aid
intuition about a value (e.g. query on pat array).  Also provide a
basis for programmer's further inferences.  Also provide a useful
degree of serendipity; draws attention to suspicious but overlooked
aspects.
** scalability
Engine has not been particularly optimized; written in Python, which
is byte compiled and then interpreted.  Ran replace on subsets of the
test cases supplied including runs over an increasing number of
randomly chosen inputs, each set being a subset of the next largest.
Tried doing it over one, but the prototype ran out of memory at one
point for 3500 inputs and another at 4500.  Could probably save space
by using a different representation or not storing every single tuple
of values every time.

Infers invariants over an average of 71 variables (6 original, 65
derived; 52 scalars, 19 sequences) per instrumentation point.  1000
test cases produce on average 10,120 samples per IP, and the system
takes 220 seconds to infer the invariants for all that.  So for 3000
test cases there are 33,801 samples and it takes 540 seconds.

Number of instrumented variables is the most important factor.  Number
is difficult to predict a priori because it depends on what other
variables are, also derivations.  Note that on average about ten
variables are derived for each original one; insensitive to relative
numbers of scalars and arrays.  Number of scalars or sequences has no
more predictive power than the total number of variables.

In figure 7: Hard to make fair comparison between points with
different sample sizes, distributions, invariants; each point then
compares inference times for two sets of variables at a single point.
All procedure exits; one set is global variables & initial argument
values, other adds final argument variables, local variables, return
value.  Omit functions whose detection runtimes were under 1s, because
not long enough to factor out measurement perturbations.

Suggests a linear relationship between ratio change in number of
variables & ratio change in runtime; doubling variables increases
runtime by about 2.5.  Number of possible binary invariants is
quadratic in number of variables at program point; binary invariants
dominate unary ones in number & cost of computation.  Test suite size
also seems to vary linearly with time required: yet, slope varies
comnsiderably.

Best independent predictor is number of pairs of values encountered by
invariant engine; unfortunately this cannot be predicted from number
of test cases.  Number of pairs of values is correlated with the
number of values but not with the ratio of scalar to sequences.
*** invariant stability
Tried to determine whether computed invariants tend to stabilize as
suite size increases.  Distinguish between interesting differences and
uninteresting differences; an example uninteresting difference is
change in range size, and also variables taking on too few values to
infer a general invariant, but where the set changes.  Many
differences are partly due to pointer values changing, or
uninitialized array elements, and other silly things like the sum of
elements of a string.
*** improvements
Where you put the instrumentation points is important; loop heads
require processing of local stuff, whereas if you wanted globals or
other large scale structures maybe module entry/exit would be enough;
also might want to instrument only parts of the program.  Also might
not consider reexamining variables that haven't changed since last
time.  Finally fewer test cases may give faster runtime (less precise
output).  Inference engine could check fewer invariants; programmer
may not be interested in trancendental arithmetic functions.
Similarly trottling complicated derived variables or adding some
complicated ones for weird expressions.  Could also involve functions
defined in program.  Type analysis could indicate which variables are
incomparable to prevent nonsensical comparisons.
*** UI
Large data set is overwhelming; have already developed a simple query
tool.  A text editor could provide a list of invariants for the
variable under the cursor; could also filter out classes of invariants
and statically obvious invariants.  Presenting invariants on demand
could avoid computing other ones.  Users could specify additional
relations or derived variables for analysis.  Ordering according to
category or usefulness could help.  Invariant diff might help
determine how a program change has affected the invariants.
* zitser04testing
** idea is to test the usefulness of static analysis tools
*** use known vulnerabilities in Open Source stuff
*** important to catch these before, as opposed to patching
**** worms indicate that patching is not working
** static analysis is important because of enormous amounts of C code
*** dynamic is expensive and usually cannot exercise all paths
*** for buffer overflows specifically:
The dynamic versions like Stackguard do catch some buffer overflows
but turn them into DoS attacks instead by shutting the server down,
which is better but not wonderful.  Safe languages can also cause this
(although IMHO this line of argument is not convincing).

Many static buffer overflow detection tools are developed: authors
know of no comprehensive evaluations.  Most are done by the tool
developers.  Detection rate for exploitable buffer overflows is
unknown and false alarms are difficult to assess.  Prior attempts at
this are unsatisfactory because of insufficient objectivity,
simplistic code, no reporting of conditional probability.  Conditional
probability is of no false alarm in a corrected program given a
detection in the vulnerable version; important to measure the ability
to discriminate between the safe and unsafe versions of the same code.

This one measures detection & false alarm rates using 14
remotely-exploitable buffer overflows noted in security reports; also
to characterize these overflows by type, and finally to have a common
collection of realistic examples for use in development.
** Static tools
ARCHER, BOON, SPLIT, UNO are all open source, PolySpace C verifier
isn't.
*** ARCHER
Uses bottom-up interprocedural analysis, has found access violations
in the Linux kernel.  Forms approximate call graph and tries to
establish ranges for function parameters that give memory access
violations symbolically.  If topmost caller is reached and any of its
triggers are satisfied, error.  Error detection is conservative; needs
strong evidence for overflows.  Uses heuristics to suppress false
positives which may hamper its ability to find bugs.  Doesn't handle
function pointers.  Prior analysis used it to analyze 2.6 million
lines of code; generated 215 warnings, 160 of which were legitimate.
*** BOON
Models string buffer manipulations with direct access and some
std. library functions.  Strings are a pair of ints; bytes allocated
and bytes used.  Integer range constraint generated for use of a
string function; constraints are collected ignoring flow and order of
statements and used to detect overflows.  Limited because only
considers strings and is flow-insensitive.  Applied to Sendmail 8.9.3,
gave 44 warnings, 4 legit.
*** PolySpace C Verifier
Commercial tool intended for embedded software.  Details fuzzy,
claimed to use abstract interpretation.  Only known evaluation ran on
NASA rover code; code had to be broken up into blocks of 20-40k apiece
because analysis doesn't scale higher.  Upper bound for false alarm
rate generated is one false alarm for every 30-60 lines of code.  Not
verified and different types not quantified.
*** SPLINT
Extends LCLINT to handle buffer overflows and other security
violations.  Several lightweight static analysis.  Requires source
annotations to do interprocedural but even without notes creation &
use of buffers and detects bounds problems.  Uses heuristics for
control flow and loop constructs in lieu of real flow analysis.
Analyzed WU-FTP: 166 warnings, 25 real.
*** UNO
Inteded to detect uninitialized variables, dereferencing null and
out-of-bound array access.  Uses compiler extension ctree to generate
parse tree, turns into CFGs, analyzed using model checker.  Doesn't
check array indices that have complicated expressions or function
calls.  Only performs checks when bound on index can be determined or
it is a constant.  Ranges are found from assignments & conditions and
combined conservatively.  Not inter-procedural.  Applied to Sendmail
and unravel and detected no array indexing errors.  Made 58 warnings
for uninitialized variables, 53 of which were real.
** test cases
BIND, WU-FTPD, Sendmail.  Use 14 most recent severe vulnerabilities;
11 were of the remote-root type.  Tried to figure out how hard it was
to use these by running Sendmail 8.12.4 (145,000 lines of code).
Splint issued many parse errors on type definitions and they were
never able to coax it into analyzing all of it.  ARCHER parsed it but
died to zero division during analysis.  PolySpace C also didn't work;
after a lot of working with the developers, analysis ran for 4 days
before dying of internal error.

This was disappointing.  Instead, made self contained model programs
by extracting as much code as needed to reproduce the vulnerability,
which could be analyzed by all tools; 90-800 lines.  Attempted to
preserve general structure and complexity when creating this
(e.g. cross-function buffers, or weird loop-fu).  Especially difficult
to do this when the thing involved multiple procedure calls.  Took 5-7
hrs per model program.  Also arranged for inputs that tickle the bug.

For each of 14 vulnerabilities, two versions constructed; bad version
and corrected version (correction done according to maintainer's
patch).  Possible that vulnerabilities still exist in good versions,
but they don't have the same vulnerability verified with above input.
*** BIND
Four serious bugs; two are memcpy size not checked, one is a negative
memcpy argument, another uses sprintf without bounds checking.  One of
them was exploited by the Lion worm.
*** Sendmail
Seven overflows: one in the RFC822 parser not keeping track of bounds
right, gecos field copied into static buffer without checking, pointer
to buffer not reset after line read, typo prevents size check, large
input byte gets cast to an error code, negative index passes size
check but underflows anyway, strncpy size read from packet but not
checked.
*** WU-FTPD
Three: several strcpys without bounds checks, an if > should be an if
>=, several unchecked strcpy and strcats.
** overflow types
They tried not to count the N similar strcpy bugs in any one model
program more than once (eg. SM-1 had overflowed the same buffer 28
times in the same manner).  Gave every model program a weight of one
and registered which bugs it saw.

Include table?  bss is uninitialized static data.  Input taint
problems can be complicated to analyze; in SM-1, must understand that
the string can have any length, then follow it through two functions
where it is copied in a fixed-length buffer.

Ultimately, simplistic approach (not interprocedural or flow
sensitive) will miss half of the vulnerabilities.
** test procedures
No annotations used.  Only modifications were for PolySpace because it
reports the error in the string library function rather than a
backtrace.  Flags are documented and were gotten with advice from
developers.
** results
*** Detections
Count the number of times a BAD line is identified; C(d).
*** False alarms
C(f).
*** last
Counted the number of times something was caught and then the OK
version has a false alarm; number of confusions, C(df).
*** summary
Want to compute the probability of detection, probability of false
alarm (line highlighted in OK version), and probability of no false
alarm given a detection.

PolySpace and Splint found a substantial fraction of the overflows
(87%, 57% respectively) while the other three generated almost none,
both also had high false alarm probabilities (near 50%).  UNO
generated no buffer overflow warnings, archer detected one, and BOON
had two confusions on SM-6 and FTP-1.

Useful systems should have P(d) > P(f).  Also want the vertical
distance to be statistically significant.  Splint's detection range is
not outside two standard deviations from the mean (not statistically
significant at the 5% confidence level from a system that assigns 43%
of all lines BAD), whereas PolySpace is.

Analysis is incomplete since those probabilities don't capture how
they might be used; need to measure ability to detect, but also to
discriminate between presence and absence of vulnerabilities; this is
the P(!f|d); for Splint and PolySpace, both continue to signal an
overflow error over half the time after it has been fixed.

Above considers stuff on vulnerable lines only, ignoring other
warnings.  Number of other warnings, true false alarms, is high;
PolySpace produces one for every 12 lines of code and Splint one for
every 46.
** explanining false alarms
Above false alarm rates are too high.  Some of them may be real
problems.  Many of them seemed genuine; usually involved overflows in
case of some complicated control flow which is subject to the contents
of an array or structure.  Model programs don't include the parse
logic, frequently.

Models are too complicated to inspect and verify that these are not
real problems.  Decided to create additional model programs to tickle
these problems; aia2 for example where the fact that it doesn't
overflow is because the contents of an array are clamped to
appropriate values and the invalid values are never used.  Another
uses a strlen that will read beyond the upper bound of non-null
terminated strings, but string is guaranteed to be null terminated
because it is initialized to a string literal.

Running this, it is verified that many of these tools generate false
alarms when buffers are accessed according to the contents of an
array; this is very common in server programming, and Splint doesn't
trip over inp but hits the other one, and PolySpace hits both.
** discussion
Simple static analysis techniques do not find buffer overflows
meaningfully.  Detection rates of three of the five are below 5%, and
the ones that did better produce high numbers of false alarms and
cannot distinguish between bad and good code.  Also PolySpace is too
slow for common use; days to analyze 100,000 lines.

Suggest development include testing of complicated buffer overflows
and will be releasing the model programs toward this end and a library
of simpler cases.  Taint analyses should be performed.

Usability is a problem; none of the best tools could handle Sendmail,
and only ARCHER was able to impersonate gcc in makefiles.
* hovemeyer04finding
** focuses on likely bugs, not really any sort of complicated analysis
*** numerous distinct detectors for "bug patterns"
Code that is likely to be an error.  They claim this approach is valid
because they find embarassing bugs in widely used apps and libraries.
*** also do false alarm anslysis
*** hope to raise awareness of low hanging fruit
*** also focus research on automatic bug finding
** other techniques
*** code inspections
Requires human cooperation, intensive, people can be distracted by
what the code is supposed to do and not what it does.
*** testing and assertions
Never consider infeasible paths; can ignore bugs that didn't come up
today for some reason; high statement or branch coverage is very
difficult; time consuming to run.
*** formal proof is best static technique
While true, so hard to do the technique is largely useless.
*** partial verification
Tries to prove that some property holds; can be complete or incomplete.
*** unsound techniques
Probable bugs, but may miss some real bugs and also may emit some
inaccurate warnings.
** bug checker vs. style checker
Line they draw is this: former looks at what the code is doing, latter
looks at what the code looks like.  Style checkers are more widely
used, probably because bug checkers tend to require more work to
interpret.  Also, percentage of false warnings tends to increase over
time; suppression of these needs to be done to keep things under
control.

They like bug pattern tools because they're easy to implement, tend to
produce output that is easy for programmers to understand, can be
effective at finding real bugs even though they don't perform deep
analysis, acceptably low rate of warnings.
** findbugs
Many different detectors, with different strategies on the bytecode
using BCEL.
*** strategies
**** class structure and inheritance heirarchy only
**** linear code scan
**** control sensitive
Use CFG
**** dataflow analysis
Use dataflow analysis
*** detector structure
None use any analysis techniques more sophisticated than in an
undergrad compiler course.  Dataflow detectors are of course the most
complex but there's a framework for it that factors out a lot of the
code.  Most detectors are 100 lines of source code, the most complex
is 1000 (including whitespace & comments).
** bug pattern detectors
*** cloneable not implemented correctly
Most common violation is not calling super.clone (); won't work for
subclasses.  Silenced if class is final.
*** double checked locking
This is a common but broken thing because it makes naive assumptions
about the memory model; some of the writes can be reordered by the
compiler or by the CPU.  Can work if the checked field is marked
volatile.

if (x == null) {
   synchronized (X.class) {
      if (x == null) {
        x = new X ();
      }
   }
}
*** dropped exception
Empty catch block; frequently indicates programmer not believing it
can occur.
*** suspicious equals
Dataflow analysis to determine when two objects of types known to be incomparable are compared using equals (); always false and usually indicates that the wrong objects are being compared.
*** covariant equals
public boolean equals (Foo obj) { ... } is wrong because it doesn't
override Object.equals, but looks like it works, and if the class is
accessed through references of the class type (not supertypes) it will
work, but the first time it gets put into a container disaster occurs.
*** equal objects have equal hashcodes
A lot of people override equals () without overriding hashCode () as
well.  Both must be done at the same time.  This is so common a lot of
Smalltalk implementations look for this, too, for the same reasons.
Hard to spot with inspection because there's nothing to see; code is
missing.
*** inconsistent synchronization
Looks for accesses to fields to determine which ones are made while
the object's lock is held, and things that are sometimes accessed with
the lock and sometimes not are problems.  Specifically exclude public
or volatile fields, fields that are never read without a lock are
ignored, access in methods unlikely to be reached when the object is
accessible to multiple threads (constructors) ignored, accesses in
nonpublic methods only called when lock is held are treated as locked.

Counts reads & writes with and without the lock.  Reports a bug if
there are some unsynchronized accesses to the field and

2(RU + 2WU) <= (RL + 2WL)  and RU + WU <= RL + WL

(RU and WU are unlocked reads and writes and RL and WL are locked
reads and writes); this is a heuristic to capture the idea that most
of the accesses are performed when a lock is held and that looking at
whether a lock is held when the write is performed is a better
indication that the synchronization was intentional rather than
incidental.

Detector uses whole program dataflow analysis, using redundant load
elimination to determine when a value loaded from a field is likely to
be the same value as that of an earlier load to get around stuff like

synchronized (x) {
   ... = x.f;
   x.g = ...;
}

so that they all use the same x.  This is a hard problem in general,
and this detector relies on the fact that synchronizing on this is a
common Java idiom.

Caught a bug in the GNU classpath java.util.Vector, and they noticed
that it is apparently common for programmers not to understand the
sync requirements of classes they are performing maintenance on and
thus introduce sync bugs.
*** static field modifiable by untrusted code
Several different ways this can occur: a static non-final field has
public or protected access, a static final field has public or
protected access and references a mutable structure, a method returns
a reference to a static mutable structure.  For example, sun's
java.awt.Cursor has a protected static non-final field predefined with
a cache of 14 predefined cursor types; untrusted code could muck with
this.  Also javax.sql.rowset.spi.SyncProvider, which has 11 public
static nonfinal int fields.
*** null pointer dereference or redundant comparison to null
Dataflow analysis, strictly intraprocedural, takes if comparisons into
account so it knows that in

if (foo == null) {
   ...
}

foo is null in the body there.  Two types are produced: null
dereferences that would be guaranteed to occur given full statement
coverage are high priority, those guaranteed to occur given full
branch coverage are medium priority (because full branch coverage is
often infeasable).

For example:

Control c = getControl ();
if (c == null && c.isDisposed ())
    return;

which was in Eclipse.  An example of the other type is this from
java.awt.MenuBar:

if (m.parent != this) {
   add (m);
}
helpMenu = m;
if (m != null) {
   ...

the first if implies that m is not null, and then the code checks it,
which is probably an indication of confusion.
*** non short circuit boolean
Using & or | instead of && or ||; probably a typo but passed on
silently.
*** open stream
Because Java finalization is unreliable it's good practice to ensure
that open streams are closed before they become unreachable.  Looks
for streams that are created (opened) in a method and not closed in
all paths out.  Uses dataflow analysis.  Streams which escape are
ignored; streams passed into a method or loaded from a field are
ignored (somebody else's responsibility), streams that are known to
not to be any real resource (byte array streams) are ignored, finally
any stream transitively created from an ignored stream is ignored
(using the stream decorators).
*** read return should be checked
java.io.InputStream has two read methods which can read into the
buffer less than the input argument; frequnetly code simply assumes
that the amount they are expecting will get read in, which is wrong.
This could be done using dataflow, but they just scan for a call to
the read method taking a byte array, followed by a POP.  If a call to
InputStream.available () (which checks the availability of data in the
stream) is made, this stuff is ignored for the next 70 warnings (a
number arrived at by studying the Sun libraries).  Ditto for skip().
*** return value should be checked
Sometimes this is programmers assuming that String objects, for
example, are mutable.  Searches for calls to any memory of a set of
methods followed immediately by POP or POP2.  The methods are any
String method returning a String, StringBuffer.toString (), any method
if InetAddress, BigInteger, BigDecimal, MessageDigest.digest(byte[]),
constructor for any subclass of Thread or Throwable.

Shows how bug checkers can dispel common misconceptions about API
semantics.
*** non serializable serializable class
Classes tagged with Serializable that contain non-transient fields
that are not Serializable or sueprclass isn't serializable and doesn't
have an accessible no-argument constructor.  If the class doesn't
implement it directly and doesn't define a serialVersionUID field,
priority is reduced (to below that reported normally).  Also if a
non-serializable instance field has an abstract or interface type
(since all concrete instances may be serializable).
*** uninitialized read in constructor
the constructor reads a field that hasn't been initialized; sometimes
results from the programmer confusing the field with a parameter.
*** unconditional waits
Looks for calls to wait () that are immediately preceded by a
monitorenter and are not the target of any branch.  Catches stuff like this:

if (!enabled) {
  try {
     synchronized (lock) {
       lock.wait ();

Implies that the condition associated with the wait is not done while
a lock is held.
*** waits outside of loops
There's a window between when the time that the thread is woken and
acquires the lock which can cause the wait condition to be false; wait
() also permits spurious wake ups.  This can be done correctly but is
rare and worth examining.
** evaluation
Some comments on reporting: no value judgements made; warnings are
just reported (example: static field that can be modified by untrusted
code is very reliably detected but whether it's *wrong* requires
judgement); some admit false positives and are reported as such; some
may be violation of good practice but not likely to cause problems,
and are reported as mostly harmless; and then there are serious bugs.

Ran it on GNU classpath 0.08, Sun libraries from JDK 1.5.0 build 59,
Eclipse 3.0, DrJava 20040326, JBoss 4.0.0RC1, jEdit 4.2pre15.  All
commercial grade products with large communities possibly excluding
GNU classpath.

None are particularly expensive; on the author's machine the whole
suite never took longer than 65 minutes on any of the programs and
some of those are very big.  Have not attempted to performance tune.

Target was that at least 50% of reported bugs should be real; fairly
close.  Accuracy varied by application: ignored read was usually
pretty reliable except in Eclipse, which uses a custom input class
where read () is guaranteed to return the number of bytes requested.
Only unconditional wait and wait not in loop were less accurate, but
they produced few warnings and due to the potential difficulty of that
stuff they decided this was okay.

Very successful finding bugs written in undergraduate courses.

All this stuff has been extensively tested and used in production
environments; a strong argument for the need for this stuff.

Insert table or summarize?
*** other detectors
FindBugs reports significantly fewer warnings than PMD which is mostly
a style checker, and a little more than KLOC.
** why do they occur?
*** everybody makes dumb mistakes
Many null pointer bugs come from using && when you meant ||, or the
equals/hashCode problem.
*** many opportunities for latent bugs in Java
For example, hashCode/equals or covariant equals, and things like the
Serializable requirements are not checked by the comipler but cause
runtime errors.
*** programming with threads is hard
** their experience
Have discovered bugs in Eclipse and some problems with the Sun JDK,
International Children's Digital Library, undergraduate assignments.
The new unbalanced locks may require additional warnings.
** user experience
GIS application; 3000 warnings reported, developers spent a week
getting it down to below 1000 implying there was enough there worth
fixing.  Project leader reports they're still working on some of the
remaining ones.  Catch a number of things where code misleads
inspection.  Also ran it on a financial application, with reasonable
success.
* 512560
** data race detection
** run time analysis
** previous algorithms have been quite precise but exact very heavy
   time overheads
*** 3 to 30 *orders of magnitude*
** others reduce the overhead at a precision cost
** key idea is weaker than relation
*** identifies accesses that are redundant for races
** overhead between 13% and 42%
** claim high precision, as well
** architecture is as follows:
*** optional static datarace analysis
**** anything not in there will never cause a datarace
*** instrumentation
**** optimization possible; sometimes the trace points are redundant
**** generates access events during execution
*** optional runtime optimizer
**** uses a cache to id and deiscard redundant access events
*** runtime detector
**** analyze the events and detects races during execution
** you really need to do everything for good performance
** could do post-mortem race detection by storing events and analyzing
   them offline
** races
*** a race is two memory accesses where:
**** two different threads
**** same memory location
**** not guarded by a common lock
**** no execution ordering enforced between the two accesses
***** mostly this has to do with start and join stuff
****** Can use a dummy synchronization object to handle joins
****** start is more difficult and ownership is used
*** formally define detection
**** access event is a 5-tuple (m, t, L, a, s)
***** m is logical memory location
***** t is the identifiy of the thread
***** L is the set of locks held
***** a is the access type (one of WRITE, READ)
***** s is the source location of the instruction
Then, IsRace(ei, ej) <=> (ei.m = ej.m) & (ei.t != ej.t) & (ei.L
intersect ej.L = emptyset) & (ei.a = WRITE | ej.a = WRITE)

Doesn't capture ownership model; discussed later
**** set FullRace to be all access pairs that form a race
***** anything hoping to get the entire set will be O(N^2) for N accesses
Obviously, as |FullRace| could be N^2.
***** they do not guarantee enumeration of all pairs
****** returns at least one race for each memory location with a race
MemRace(mk) = {<ei, ej> \in FullRace | ei.m = ej.m = mk}
IsRaceOn(ei, m) <=> \exists ej. <ei, ej> \in MemRace(m)
*** also support some debug thing
** actual algorithm
*** weaker-than
**** if \forall future ek, IsRace(ej, ek) => IsRace (ei, ek), don't need to consider ej
We say that ei is weaker than ej, and can determine that this occurs
dynamically.
**** t_bot is a possible value of e.t for access events e
It means at least two distinct threads.  We set ei.t = t_bot when
there is some later event ej such that ei.m = ej.m & ei.L = ej.L &
ei.t != ej.t.  The idea is that any other access with a
nonintersecting lockset is a race (unless everything is a read).  It
is a space optimization, but also implies that they cannot always
report the specific earlier thread in a data race.
**** partial order \sqsubseteq
ti \sqsubseteq tj <=> ti = tj \vee ti = t_bot
ai \sqsubseteq aj <=> ai = aj \vee ai = WRITE
for access events p and q,
p \sqsubseteq q <=> p.m = q.m \wedge p.L \subseteq q.L \wedge p.t \sqsubseteq q.t \wedge p.a \sqsubseteq q.a
and
p \sqsubseteq q => (IsRace (q, r) => IsRace (p, r))
**** idea is that if p is weaker than q, only store information about the weak
There is a danger that if the tool reports a spurious race, it may
hide a real one.  Their solution is to use extra locks to remove
spurious races.
** algorithm
*** trie-based
Put history of accesses to that location with an edge-labeled trie.
Edges are IDs of lock objects and nodes hold thread and access type
info for possibly empty set of events.  Locks for the access is the
path from the root to the node.

Nodes have a thread field t and access type field a, internal nodes
with no accesses have type READ and thread t_top (meaning no threads).
**** meet (on threads and access info)
\forall i. ti \sqcap ti = ti, ti \sqcap t_top = ti, a_i \sqcap a_i = a_i
\forall i, j. ti \sqcap tj = t_bot if t_i != t_j
\forall i, j. ai \sqcap aj = WRITE if ai != aj
**** process
Upon encountering an event e, first check if there is an access such
that e_p \sqsubseteq e; walk the trie for e.m following only edges
with identifiers in e.L (using DFS).  If any encountered node is
weaker than e, , forget about e.  Catches most accesses.

If not found, check e for dataraces with another DFS of trie; for each
node n, three cases.  After that in all cases update the trie; if
there is an n whose path to the root is labelled with e.L, n.t <- n.t
\sqcap e.t and n.a <- n.a \sqcap e.a.  If no n, add a new node for
this one.  And walk the trie once more to remove everything stronger
than this new one.
***** last lock checked is in e.L
Then e shares a lock with everything in n or below and no races are
possible; prune search.
***** last lock not in e.L, e.t \sqcap n.t = t_bot and e.a \sqcap n.a = WRITE
Now we have a race; intersection of lock sets is empty, e.t differs,
at least one access was a write.  Report immediately and terminate
traversal.
***** otherwise
Check n's children
** implementation
Done in Java with allegedly straightforward code.  Doesn't handle GC
move events well; they handle this in the prototype by using more
memory so no GC.
** optimizations
*** making weaker-than cheaper
**** two caches, one for read, one for write, per thread
Indexed by memory location.  We guarantee that if an entry is found
there has been a weaker access already recorded; otherwise add the new
info and update the cache.  We know the new event's current thread.
Different access rules guarantees that the access type will be the
same.  Guarantee that p.Locks is a subset of currently held locks for
all p in the cache by, on monitorexit for lock l, evicting anything
where l \in p.Locks.  Note that due to reentrancy on Java
synchronization blocks, we only consider the most enclosing block.
Cache is indexed by memory location; all other entries are weaker than
the thing being looked up and we're fine.

The eviction process can be slow.  They use the nested locking
discipline found in Java; so for each lock held by the thread, keep a
linked list of cache entries where l was the last lock in the entry to
be acquired; then when l is released these things are all evicted.
Doubly linked list so that individual entries can be removed after
eviction (due to cache conflicts).
** static analysis
*** goal is to determine all statements that might cause a race
*** again with the formalism
(we ignore the execution ordering and assume it always holds)
IsMayRace (x, y) <= AccMayConflict (x, y) & (! MustSameThread (x, y)) & (! MustCommonSync (x, y))
AccMayConflict (x, y) = true if x and y may access the same memory location; use may points-to for this.
MustSameThread (x, y) = true if they are always executed by the same thread; use must points-to
MustCommonSync (x, y) = true if x and y are always synchronized; must points-to again.
*** interthread control flow graph
Detailed interprocedural representation of the multithreaded program;
nodes instructions, edges are sequential and parallel flow.  Each
method and each sync block has distinguished entry and exit nodes.
Contians four types of control flow edges: intraprocedural (which
capture all intraporcedural control flow including exceptions), call,
return, start.  Start has to do with threading, of course.  Join edges
are not included because they are unnecessary for the analysis.
*** points-to
Flow insensitive, whole program.  Distinct abstract object for each
allocation site; each one represents all concrete objects created at
that same site.  Precise analysis is expensive; simple and
conservative is used, based on the notion of single-instance
statements, each of which executes at most once per execution.
Objects created there are single instance objects.  If an access
poitns only to one abstract object and it is a single instance object
then it is a must points-to relation (using a special null object for
null references).  More details elsewhere.

AccMayConflict(x, y) = (MayPT(x) \cap MayPT(y) \neq \emptyset) \wedge (field(x) = field(y))
Where field is the accessed field of the object or class.
ThStart(u) is the set of thread-root nodes from whose entry nodes there is an intrathread ICFG path to u.
MustThread(u) = \bigcap_{v \in ThStart(u)} MustPT(v.this)
MustSameThread(x, y) = MustThread(x) \cap MustThread(y) \neq \emptyset
(v.this is the this pointer of the thread-root node v)

For n \in ICFG, Synch(n) = true if n is a synchronized method or
block, and u_n is the access of the synchronization object in that
case.  Pred(n) is the set of intrathread predecessor nodes of n in the
ICFG.  Then,

Gen(n) = \left\{\begin{array} MustPT(u_n) & \text{if $Synch(n)$} \\
                              \emptyset   & \otherwise \end{array}\right.
SO^n_o = SO^n_i \cup Gen(n), SO^n_i = \bigcap_{p \in Pred(n)} SO^p_o
MustSync(v) = SO^n_o \forall v \in n
MustCommonSync(x, y) = MustSync(x) \cap MustSync(y) \neq \emptyset
*** escape analysis
Want to ID objects as thread-local because those can never participate
in data races.  Call an object thread specific to T if all accesses
occur while T is being constructed or by T itself.  Want to permit
people to store objects in fields of T, even though the constructor
could concievably access them.  Use an approximation:
**** thread-specific methods are either:
***** <init> methods or run methods that are not invoked explicitly on threads
***** non static method all of whose direct callers are thread-specific non-static with the same this value
**** thread specific fields
Objects only accessed by getfield/putfield on the this reference of a
thread specific method.
**** unsafe thread
Thread whose execution may start before its initialization completes;
we decide a thread is unsafe if its constructor can transitively call
Thread.start or the this reference escapes from the constructor.
**** thread specific
Object O is thread specific to T if T is safe and O is only reachable
from thread-specific methods of T or through thread-specific fields of
T.  Accesses to a thread specific object of a safe thread cannot be a
race; accesses to thread-specific fields cannot be a race.
** compile time optimization
Goal is to avoid inserting unnecessary instrumentation.
*** static weaker-than relation
Events(S) is the access events generated by instrumentation statement
S.  Si is weaker than Sj, Si \sqsubseteq Sj, if \forall ej \in
Events(Sj) \exists s_i \in Events(Si) in the same execution such that
ei \sqsubseteq ej and there is no thread start or join between ei and
ej.

have conservative and effective analysis for computing Si \sqsubseteq
Sj when they belong to the same method (more sophisticated
interprocedural would be needed for arbitrary Si Sj).  Use a pseudo
instruction trace(o, f, L, a) for the access event instrumentation (o
is object, f is field, L is locks held, a is access type).  All
operands are treated as uses of their values.  For static fields, o is
the class, and for array elements, f is the index.  Do not attempt to
optimize acros thread boundaries, so don't record that.  Insert one of
these after every instruction which accesses a field.  After insertion
we attempt to eliminate them with the static weaker-than.

Exec (Si, Sj) is true if Si is on every intraprocedural path that
contains Sj and there is no method invocation on any intraprocedural
path between them.  They used domination to compute it; you could use
post-domination but because almost anything can throw an exception in
Java it is very hard to prove.  now,

Si \sqsubseteq Sj <= Exec (Si, Sj) \wedge a_i \sqsubseteq a_j \wedge
outer(Si, Sj) \wedge valnum (o_i) = valnum (o_j) \wedge f_i = f_j

valnum is the value number of the object reference.  outer (Si, Sj) is
true iff Sj is at the same nesting level in synch. blocks as Si or at
a deeper level within Si's block.  The other bit is to show that the
memory locations are the same.  (value number has to do with SSA)
** implementation
*** instrumentation and analysis
Performed during compilation of the method by Jalapeno.  Created a new
instruction in the high level rep of the compiler for the trace
instruction.  After that, SSA conversion, dominance relation is
computed, eliminationg of redundant traces is done using an existing
value numbering phase, remaining trace statements are marked so they
will not be eliminated as dead code unless really truly dead.

After this, expand each trace into a call to the method of the dynamic
detector and force Jalapeno to inline the call; it then optimizes the
HIR again.
*** loop peeling
Loops source of redundant access events; hard to statically eliminate
because the static weaker-than relation cannot be applied
(insufficiently powerful) and standard loop invariant motion cannot be
used because the statement may throw an exception (very common in
Java).  To do this, unroll the first iteration of the loop (call it
loop peeling); now the inner trace statement is statically weaker and
good to go.
** ownership model
Idea is that for each memory location the owner is the first thread to
access it.  For each access if the current thread owns the location we
ignore it.  The first time the location is not accessed by the owner
it becomes shared: t_o = \bot and all subsequent access events are
sent to the rest of the detector.  Essentially access stream filtered
to only include shared state operations.

This requires that the weaker-than relation change; if e1 is setn to
the detector while e1.m is owned, then e1.m changes to shared, e2
(stronger) should not be suppressed.  In the cache this is dealt with
by evicting the location from each thread's cache when it is shared.
Harder to avoid in compile time optimization.  Only truly sound
approach is to use post-domination, but that's very hard in Java.
Actual approach is to ignore it for static and dynamic.  This means
the tool may suppress races, but they claim that this hasn't been a
problem in their experiments.
** experimental results
Use some third party stuff; a multithreaded ray tracer, travelling
salesman, successive overrelaxation, real time discrete event
simulator, web crawler. (mtrt is SPECJVM98)

The ray tracer records so many accesses that Jalapeno runs out of
memory before it even runs unless static analysis is done; then it's
fine.  Effectiveness of optimizations is very strange; cache is
totally essential for the TSP problem, static analysis is important
for TSP and critical for ray traceer, dominators and peeling important
for SOR with cache less so and static totally unimportant.

Accuracy: ray tracer reports two races, both of which are legitimate
although one is benign because value in question is never read.  TSP
has serious datarace, and also additional races that are in fact
spurious due to higher level synchronization.  SOR races are not
really unsynchronized; program uses a different sort of sync which
isn't captured by the algorithm (why run it, then?!).  Web crawler has
several unsynchronized accesses (size of thread pool is read & written
w/o appropriate locking, unsynchronized null assignment which can
cause the program to die at times (previous work believed this
actually to be benign, but it isn't)).  Distinguishing fields is
frequently significant, and turning off ownership reports many
spurious dataraces when data is initialized by one thread and passed
to another for processing (particularly in SOR).

Prior work like Eraser uses a looser definition of data races, which
produces spurious race reports in mtrt due to complicated IO
statistics locking (three threads, two locks each, each pair shares
one, but mutual intersection is \emptyset).
* 781169
** A type system for atomicity
*** wang03runtimeRV alludes to this for deadlocks and races
** Race free does not mean safe; cites example
(defvar x)
(defvar l)
(defun m ()
  (let ((t (synchronized (l) x)))
    (incf t)
    (synchronized (l) (setf x t))))
** They propose something else, which is atomicity
*** Same atomicity as wang03runtimeRV
*** Property of a method
*** they claim it's stronger, which I'm not sure I believe
**** Yeah, wang03 shows that they're incomparable.
*** Claim it's a natural property for a method
** Add a new keyword atomic, which verifies that the method is atomic
*** Try doing this using Java classes
**** Found some subtle violations in java.lang.String and StringBuffer
     that are not due to races
** Also add this let and fork business
*** I think this is just to rationalize Java's threading semantics,
    it's not significant
*** Gives some motivating examples
** And now we get into left/right/both/non movers.
*** Important: the moving here is between *different* threads
*** We get the (R+B)*N?(L+B)* is sufficient for atomicity again
** They have this ConcurrentJava which is just Java with some
   syntactic suger, it appears.
*** fork is like new Thread (new Runnable () { ... }).start () but
    short.
*** Actually smaller than Java but the one can be translated to the
    other so it's more convenient for atomicity purposes
** Types
*** const: constant
*** mover: wang's left/right/both
**** since Java doesn't have separate acquire/release, left/right
     mover isn't important
*** atomic: single atomic action (primitives)
*** cmpd: none of above apply
*** error: something else
*** adds iterative closure (execute the thing an arbitrary number of
    times
*** ordering and join operation
*** also a sequential composition operator
*** also a conditional based on whether a lock is held: l ? mover :
    error
*** further extend \subseteq to handle the locks held and not held
*** another type: add guarded_by and write_guarded_by to fields
**** (first is read and write, second just write)
**** If neither is true, then read/write at any time
*** method may require locks to be held, through requires declaration
*** define the way it interacts with the statements in the language
**** largely the way you'd assume it would
**** P, E |- e : t & a  e expr, t type, a atomicity
**** insists the lock be a constant expression, but this isn't
     necessarily bad
**** Generally EXP SYNC is complex and should be expanded upon
** They implemented it for the full Java language
*** Built on rccjava
*** arrays raised some technical challenges
**** introduced some unsafe casts that could be statically checked by
     adding linear, unique or ownership types
*** found an atomicity error in StringBuffer.append (StringBuffer sb)
**** a second thread could remove characters from sb after the method
     figures out the length
*** similar problem in String.contentEquals (StringBuffer sb)
*** Rep exposure problem [15]
**** they suggest ownership type systems or escape analysis for
     reasoning about it being useful for verifying atomicity
*** Vector proves that the conditional atomicities are totally
    necessary
*** java.net.URL shows a number of races
**** may be benign
* wang03runtimeRV
** Idea is to show that shared variable programs do not have deadlocks or races
*** Type systems are an effective way to do both of these
**** Require onerous manual annotation
**** Undesirably restrictive
*** These can be checked by run-time analysis
**** For example, lockset and goodlock
**** Less powerful, but automatic
**** Indicate whether this trace has a problem and whether others are suspicious
** Paper proposes specialized run-time analysis for atomicity
*** Basically the I in ACID; not the A
*** Atomicity is defined in terms of equivalent
**** Every concurrent execution is equivalent to some serial execution
*** A natural API property
*** Could be done with types
**** Annotation requirement is small but needs to have race condition annotations
**** Flanagan and Qadeer
*** First algorithm checks how locks are used ot protected shared variables
**** Computes commutativity properties of events
*** Second is checking atomicity violations by legal permutations of events
** Race freedom and atomicity are incomparable (neither implies the other)
** View consistency in high level data races
*** Seems similar, but also incomparable
** Some formal definitions
** Reduction based
*** Have left movers, which can commute to the left
*** Ditto right, both movers
*** These commutativity properties allow us to establish atomicity
**** In fact all transactions of the form (R+B)*N?(L+B)* are atomic
***** N is non-movers
***** This is a conservative test for atomicity; converse doesn't hold
**** Acquire are right movers, release events left, race free reads & writes are both
***** To check whether the access is race free, consider locks held
** Other algorithm is block based
*** More expensive, significantly more precise
*** First: compute feasible interleavings
**** Can check that systems are free of deadlock potential using goodlock
**** After that, there's a simple test
**** Deadlocks make this a lot uglier, still possible; future work
*** Now: algorithm for transactions accessing one variable
**** tries to prove T not atomic by looking for small patterns that can be interleaved badly
***** Read from one between two writes in another
***** Write between two reads
***** Write between a write and a read
***** Final write between a read and subsequent write
**** Block is a pair of read or write events from a single transaction, with sync information needed for interleavings
**** Algorithm:
(defun is-atomic-1-var (transactions)
  (dolist (t transactions)
    (dolist (t' transactions)
      (unless (eq (thread t) (thread t'))
	(dolist (b (blocks t))
	  (dolist (b' (blocks t'))
	    (unless (is-atomic-block b b')
	      (return-from is-atomic-1-var nil)))))))
  t)

Here is-atomic-block just checks the above conditions.  Runtime is
O(EL + B^2) where E and B are events & blocks in all transactions and
L is the max # of locks in one transaction.
*** Next: two transactions, many variables
**** Must consider blocks that contain operations on two different variables (2-blocks)
**** Algorithm:
(defun is-atomic-2-trans (t t')
  (if (eq (thread t) (thread t'))
      t
    (progn (dolist (x (union (variables-accessed t)
			     (variables-accessed t')))
	     (unless (is-atomic-1-var (proj t x)
				      (proj t' x))
	       (return-from is-atomic-2-trans nil)))
	   (dolist (b (2-blocks t))
	     (dolist (b' (2-blocks t'))
	       (unless (is-atomic-2-block b b')
		 (return-from is-atomic-2-trans nil))))
	   t)))

Here proj is the protection of transaction t on variable x (that is,
keep synchronization events and accesses to x and discard all other
variables), and is-atomic-2-block is another systematic exhaustion of
available choices.  Enumerated in WS03.
*** General algorithm
**** Complicated by cyclic dependencies
**** New algorithm is:
if UR-FW(T) is the set of transactions obtained from T by discarding
all events other than synchronization events and uninitialized reads
and final writes on shared variables:
(defun is-atomic-trans (transaction)
  (dolist (t transactions)
    (dolist (t' transactions)
      (unless (eq (thread t) (thread t'))
	(unless (is-atomic-2-trans t t')
	  (return-from is-atomic-trans nil)))))
  (all #'serializablep (every-feasible-interleaving (ur-fw transaction))))

Which is just another systematic exhasution algorithm, expensive but
may be better than a naive algorithm, and subsets that access disjoint
subsets of variables can be analyzed independently.
** Example
*** They ran it on Hashtable and found a problem with the fail fast iterator
**** h.remove may cause an iterator not to throw the exception on next
**** General problem is NP complete
* henzinger03software
** Big thing is lazy abstraction
*** idea is that we use SLAM's check/refine loop, but make it quicker
**** With the problem that that ``algorithm'' may not terminate
*** If no path to error label, we're good
*** Otherwise, check to see if it's feasible immediately
*** If infeasible, refine.
** Avoids repetitive work
** C programs
** Represented as control flow automata
*** Control flow graphs with operators on edges
*** First, build a reachability tree
**** Each node is a vertex of the CFA and a reachable region formula
**** Initially formula is empty
*** If an error node is reachable, check too see if it's real
**** If it's not, ask a theorm prover to suggest new predicates
**** Add these new predicates only in the smallest subtree containing the error
** Benefits
*** Abstracts only the reachable portion
*** Different precisions at different parts of the state space
**** means processing fewer predicates
*** Avoid redoing work we know is fine
*** Invariants can be found and a formal proof of correctness can be constructed
**** See ``Temporal-safety proofs for systems code''
** Actual implementation
*** Runs on a C program
*** Produces either an error trace or an ELF executable with an attached proof of correctness (or fails to terminate)
*** Handles all of C syntactically: pointers, structures, procedures
**** However models integer arithmetic as infinite precision and logical memory
***** Nothing that changes the layout pattern, no partially overlapped objects, pointer arithmetic respects array bounds
**** Also don't handle recursive function as yet
*** More discussion of implementation
**** A region is a tuple of CFA state, data state, stack state
**** CFA state represented explicitly
**** Data state is boolean formulas over the abstraction predicates
**** Stack state is a sequence of CFA states
**** Boolean formulas stored as BDDs
**** Strongest postcondition \& weakest precondition, as expected
**** Counterexample analysis is done by ``iterating the concrete predecessor or successor operators and checking for unsatisfiability''
**** Refinement takes an infeasible counterexample trace and feeds it to a proof generating theorem prover, then takes the atomic formulas appearing in the proof
** Optimizations:
*** Theorem proving is the most problematic
**** Fast abstract successor operation which is less precise than it could be but usually strong enough
**** only invoke the theorem prover on predicates that are affected by the statement
**** Keep only satisfiable disjuncts
**** Remove predicates that relate to variables not in scope
**** Region inclusion is performed entirely at the boolean level without interpreting the predicates
*** Means it runs on several thousand lines of C code in a few minutes
** Experience:
*** Frequently we only need a model rather than the whole thing
**** They cite the spinlocks in the Linux kernel, where they need only a model rather than the actual assembly code
***** Implement this using stub functions
*** Nondeterministic choice is useful as a modeling tool so they added explicit support for it
*** Sometimes predicate discovery isn't strong enough and so they take programmer specified predicates
**** Efficient discovery of good predicates is still an issue
**** They use numerous heuristics
*** Have run it on several large C programs, largely device driver examples from Windows or parts of Linux
**** Have found several bugs and also verified that other drivers correctly implement the spec

* arch-tag: 1BE48746-2E59-11D9-94EC-000A957284DA

